{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71e03e3-e18b-4923-8fb3-25ce3961abfd",
   "metadata": {},
   "source": [
    "# PSP Automated Magnetic Hole Finder\n",
    "## By: Robert Alexander + Jaye Verniero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e5ed-dfa5-40f5-9949-079fff99d430",
   "metadata": {},
   "source": [
    "### 0.1) Import Packages, Define Helper Functions, and Set Save Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587ec3b-a1ac-47f5-8d73-57152ac4a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Control + Shift + [ and ] will move left and right in the tabs.\n",
    "\n",
    "# Step 1: Import the modules explicitly so you can reload them\n",
    "import data_management\n",
    "import data_audification\n",
    "import time_management\n",
    "import plotting\n",
    "import buttons\n",
    "import MH_format_output\n",
    "import MH_helper_functions\n",
    "import asymmetry_calc\n",
    "import multiAvg_calc\n",
    "import stdev\n",
    "import hole_angle_calc\n",
    "import printing\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Step 2: Reload the modules to update their contents\n",
    "importlib.reload(data_management)\n",
    "importlib.reload(data_audification)\n",
    "importlib.reload(time_management)\n",
    "importlib.reload(plotting)\n",
    "importlib.reload(buttons)\n",
    "importlib.reload(MH_format_output)\n",
    "importlib.reload(MH_helper_functions)\n",
    "importlib.reload(asymmetry_calc)\n",
    "importlib.reload(multiAvg_calc)\n",
    "importlib.reload(stdev)\n",
    "importlib.reload(hole_angle_calc)\n",
    "importlib.reload(printing)\n",
    "\n",
    "# Step 3: Bring the updated functions, classes, and variables into the current namespace\n",
    "from data_management import *\n",
    "from data_audification import *\n",
    "from time_management import *\n",
    "from plotting import *\n",
    "from buttons import *\n",
    "from MH_format_output import *\n",
    "from MH_helper_functions import *\n",
    "from asymmetry_calc import *\n",
    "from multiAvg_calc import *\n",
    "from stdev import *\n",
    "from hole_angle_calc import *\n",
    "from printing import *\n",
    "from collections import Counter\n",
    "\n",
    "#Import Libraries\n",
    "# Scientific Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from matplotlib import ticker, cm\n",
    "from scipy.io import readsav, wavfile\n",
    "from scipy.io.wavfile import write\n",
    "from scipy import interpolate\n",
    "import bisect\n",
    "import json\n",
    "\n",
    "# PySPEDAS and PyTPlot Libraries\n",
    "import pyspedas\n",
    "from pyspedas import time_string, time_double, tinterpol, tdpwrspc\n",
    "import pytplot\n",
    "from pytplot import (\n",
    "    tplot, store_data, get_data, tlimit, xlim, ylim, tplot_options, options,\n",
    "    split_vec, cdf_to_tplot, divide, tplot_names, get_timespan, tplot_rename,\n",
    "    time_datetime\n",
    ")\n",
    "\n",
    "# System and File Operations\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import cdflib\n",
    "from datetime import datetime, timedelta\n",
    "from tkinter import Tk, filedialog\n",
    "import pickle #for saving data locally\n",
    "import time\n",
    "\n",
    "# IPython Widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Warnings Handling\n",
    "from warnings import simplefilter\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"IPython.core.pylabtools\")\n",
    "\n",
    "# Global Variables\n",
    "global save_dir, trange, trange_start, trange_stop\n",
    "\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f'{current_time} - \ud83d\udcda libraries imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cde94-016d-4456-8f60-55c8740d1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Set The Global Save Directory\n",
    "# #Running this code will trigger a pop-up menu that will ask you to specify a directory where you'd like to save your files.\n",
    "# #If you don't see a pop-up check your dock for a bouncing white python page and click that.\n",
    "# #Each time you change this directory you'll need to re-run the cells that generate plots before they'll recognize the new location.\n",
    "# #If this directory is not specified the code will not run properly.\n",
    "\n",
    "# # Set The Global Save Directory\n",
    "# global save_dir\n",
    "# last_dir_file = \"last_selected_dir.txt\"  # Path to save the last selected directory\n",
    "# save_dir = set_save_directory(last_dir_file)\n",
    "\n",
    "# show_directory_button(save_dir) # Display the button to show the directory\n",
    "\n",
    "# current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# print(f'{current_time} - \ud83d\udedf Save Directory Set: {save_dir}')\n",
    "\n",
    "# #\ud83d\udc47 Your save directory is confirmed here and you can click the button to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb958ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your magnetic_hole_finder notebook (e.g., \u2b50\ufe0f MH_Scan_V6.5_MAIN.ipynb)\n",
    "\n",
    "# Import Plotbot's print_manager\n",
    "try:\n",
    "    from plotbot import print_manager as plotbot_pm # Use an alias to be clear\n",
    "    # Set desired levels\n",
    "    plotbot_pm.show_error = True    # Default is usually True\n",
    "    plotbot_pm.show_warnings = True  # Default is usually True\n",
    "    plotbot_pm.show_status = False  # Example: turn off status messages\n",
    "    plotbot_pm.show_debug = False   # Example: turn off debug messages\n",
    "    plotbot_pm.show_datacubby = False # Example: turn off DataCubby messages\n",
    "    plotbot_pm.show_processing = False\n",
    "    # ... set other plotbot_pm flags as needed ...\n",
    "    print(\"Successfully configured Plotbot's print_manager.\")\n",
    "except ImportError:\n",
    "    print(\"Could not import Plotbot's print_manager. MH specific prints will use fallback.\")\n",
    "    # If Plotbot's print_manager can't be imported, the FallbackPrintManager\n",
    "    # inside data_management.py will be used for messages from that file,\n",
    "    # which prints ERROR, WARNING, STATUS by default.\n",
    "\n",
    "# Now, when your code in magnetic_hole_finder/data_management.py runs \n",
    "# (e.g., when download_and_save_data is called), \n",
    "# it will try to use the plotbot_print_manager instance that you've just configured.\n",
    "\n",
    "# ... rest of your notebook code for magnetic hole finding ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05533dd-9737-4beb-8aae-4cc705d854e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1) Find Magnetic Holes Through an Automated Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc050ae9-3b13-4c3b-b6cc-718b09e7395a",
   "metadata": {},
   "source": [
    "## DOING SOMETHING EPIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a8f80-284f-4e06-9ab7-248af520cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2728\u2728\u2728ShInY NeW wOrKiG cOdE cOdE\u2728\u2728\u2728 \n",
    "global save_dir\n",
    "\n",
    "import importlib\n",
    "\n",
    "#region Import Modules Block\n",
    "from collections import Counter\n",
    "import asymmetry_calc\n",
    "importlib.reload(asymmetry_calc)\n",
    "from asymmetry_calc import *\n",
    "import time_management\n",
    "importlib.reload(time_management)\n",
    "from time_management import *\n",
    "import data_management\n",
    "importlib.reload(data_management)\n",
    "from data_management import *\n",
    "import plotting\n",
    "importlib.reload(plotting)\n",
    "from plotting import *\n",
    "import data_management\n",
    "importlib.reload(data_management)\n",
    "from data_management import *\n",
    "import data_audification\n",
    "importlib.reload(data_audification)\n",
    "from data_audification import *\n",
    "import plotting\n",
    "importlib.reload(plotting)\n",
    "from plotting import *\n",
    "import zero_crossing_analysis\n",
    "importlib.reload(zero_crossing_analysis)\n",
    "from zero_crossing_analysis import analyze_derivative_zero_crossings\n",
    "import MH_format_output\n",
    "importlib.reload(MH_format_output)\n",
    "from MH_format_output import *\n",
    "\n",
    "import hole_angle_calc\n",
    "importlib.reload(hole_angle_calc)\n",
    "from hole_angle_calc import *\n",
    "\n",
    "\n",
    "# from hole_counter import hole_counter\n",
    "#endregion  \n",
    "\n",
    "# Initialize the counter\n",
    "hole_counter = Counter()\n",
    "\n",
    "def detect_magnetic_holes(trange, smoothing_window_seconds, min_max_finding_smooth_window, mean_threshold, search_in_progress_output, Bave_scan_seconds, break_for_assymettry, small_threshold_cross_flag, small_threshold_cross_adjustment, break_for_small_threshold_cross, break_for_complex_hole, derivative_window_seconds, OUTPUT_ZERO_CROSSING_PLOT, SEARCH_IN_PROGRESS_OUTPUT, INSTRUMENT_SAMPLING_RATE, use_calculated_sampling_rate):\n",
    "    global hole_counter  # Add this line to use the global hole_counter\n",
    "    \n",
    "    max_window_seconds = smoothing_window_seconds\n",
    "    \n",
    "    # \ud83d\udcc8 Data Preparation Step 1: Extend the Time Range\n",
    "    extended_trange = extend_time_range(trange, max(smoothing_window_seconds, min_max_finding_smooth_window))\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 2: Download Data for the Extended Range\n",
    "    times, br, bt, bn, bmag_extended = download_and_prepare_high_res_mag_data(extended_trange)\n",
    "    print(f\"length of bmag_extended is {len(bmag_extended)}\")\n",
    "\n",
    "    # Calculate sampling rate based on the extended data\n",
    "    total_samples = len(bmag_extended)\n",
    "    start_time = datetime.strptime(extended_trange[0], '%Y-%m-%d/%H:%M:%S.%f' if '.' in extended_trange[0] else '%Y-%m-%d/%H:%M:%S')\n",
    "    end_time = datetime.strptime(extended_trange[1], '%Y-%m-%d/%H:%M:%S.%f' if '.' in extended_trange[1] else '%Y-%m-%d/%H:%M:%S')\n",
    "    duration = end_time - start_time\n",
    "    duration_seconds = duration.total_seconds()\n",
    "\n",
    "    if use_calculated_sampling_rate:\n",
    "        INSTRUMENT_SAMPLING_RATE = total_samples / duration_seconds\n",
    "        print(f\"\u2733\ufe0f Using calculated SR of {INSTRUMENT_SAMPLING_RATE:.2f} Hz\")\n",
    "    else:\n",
    "        print(f\"\u2733\ufe0f Using predefined SR of {INSTRUMENT_SAMPLING_RATE} Hz\")\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 3: Apply smoothing to the bmag data for detection and maxima finding\n",
    "    bmag_slow_smooth_extended = efficient_moving_average(times, bmag_extended, smoothing_window_seconds, determine_sampling_rate(times, INSTRUMENT_SAMPLING_RATE, True), mean_threshold)\n",
    "    bmag_fast_smooth_extended = efficient_moving_average(times, bmag_extended, min_max_finding_smooth_window, determine_sampling_rate(times, INSTRUMENT_SAMPLING_RATE, True), mean_threshold)\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 4: Clip the smoothed data to the original time range\n",
    "    times_clipped, bmag = clip_to_original_time_range(times, bmag_extended, trange)\n",
    "    _, bmag_slow_smooth = clip_to_original_time_range(times, bmag_slow_smooth_extended, trange)\n",
    "    _, bmag_fast_smooth = clip_to_original_time_range(times, bmag_fast_smooth_extended, trange)\n",
    "\n",
    "    print(f\"Post-clipping length of bmag is {len(bmag)}\")\n",
    "\n",
    "    # # Calculate sampling rate\n",
    "    # total_samples = len(bmag)\n",
    "    # start_time = datetime.strptime(trange[0], '%Y-%m-%d/%H:%M:%S.%f' if '.' in trange[0] else '%Y-%m-%d/%H:%M:%S')\n",
    "    # end_time = datetime.strptime(trange[1], '%Y-%m-%d/%H:%M:%S.%f' if '.' in trange[1] else '%Y-%m-%d/%H:%M:%S')\n",
    "    # duration = end_time - start_time\n",
    "    # duration_seconds = duration.total_seconds()\n",
    "\n",
    "    # if USE_CALCULATED_SAMPLING_RATE:\n",
    "    #     calculated_sampling_rate = total_samples / duration_seconds\n",
    "    #     INSTRUMENT_SAMPLING_RATE = calculated_sampling_rate\n",
    "    #     print(f\"\u2733\ufe0f Using calculated SR of {INSTRUMENT_SAMPLING_RATE:.2f} Hz\")\n",
    "    # else:\n",
    "    #     print(f\"\u2733\ufe0f Using predefined SR of {INSTRUMENT_SAMPLING_RATE} Hz\")\n",
    "\n",
    "    \n",
    "    magnetic_holes = []\n",
    "    hole_minima = []  # To store the minima for plotting\n",
    "    hole_maxima_pairs = []  # To store the left and right maxima pairs for plotting\n",
    "    magnetic_hole_details = []  # To store details of each detected magnetic hole\n",
    "    i = 0\n",
    "\n",
    "    #----- \u2705\u2705\u2705 1: Scan through the data to detect drops below the slow smoothed value-----//\n",
    "    \n",
    "    while i < len(bmag):   \n",
    "        while i < len(bmag) and bmag[i] > bmag_slow_smooth[i]:  # skip over where bmag > bmag_slow_smooth\n",
    "            i += 1\n",
    "        if i >= len(bmag):\n",
    "            break    \n",
    "        L_threshold_cross = i\n",
    "        hole_counter['potential'] += 1\n",
    "        \n",
    "    #----- \u2705\u2705\u2705 2: Continue scanning forward until bmag rises above the slow smoothed value-----// \n",
    "        #region Right Threshold Cross Block\n",
    "        while i < len(bmag) - 1 and bmag[i] < bmag_slow_smooth[i]:\n",
    "            i += 1\n",
    "        R_threshold_cross = i\n",
    "\n",
    "        if R_threshold_cross - L_threshold_cross <= small_threshold_cross_flag:\n",
    "            hole_counter['small_threshold_cross'] += 1\n",
    "            if break_for_small_threshold_cross:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this small threshold cross.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "            else:\n",
    "                R_threshold_cross += small_threshold_cross_adjustment\n",
    "                L_threshold_cross -= small_threshold_cross_adjustment\n",
    "                print(f\"\ud83d\udd25 Hole detection threshold crossed near the bottom of a hole, expanding L & R thresholds for analysis:\")\n",
    "                print(f\"New L_threshold_cross: {L_threshold_cross}\")\n",
    "                print(f\"New R_threshold_cross: {R_threshold_cross}\")\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705\u2705\u2705 3: Scan for minimum where bmag[i] < slow smoothed value-----//\n",
    "        min_idx = np.argmin(bmag[L_threshold_cross:R_threshold_cross + 1]) + L_threshold_cross  # Offset to slice start, end is exclusive so +1\n",
    "        min_value = bmag[min_idx]\n",
    "        print(f\"Minimum initially identified at {min_idx}\")\n",
    "    \n",
    "    #----- \u2705 4: Find the left peak (maximum) before the drop using the fast smoothing window-----//\n",
    "        #region Left Peak Scan Block\n",
    "        # Move left while the current smoothed value is lower than the previous smoothed value\n",
    "        L_plateau_scan = L_threshold_cross\n",
    "        while L_plateau_scan > 0 and bmag_fast_smooth[L_plateau_scan] < bmag_fast_smooth[L_plateau_scan - 1]:\n",
    "            L_plateau_scan -= 1\n",
    "        L_avg_inflect = L_plateau_scan\n",
    "\n",
    "        # If the identified inflection point is too close to the threshold cross, scan further back in time\n",
    "        if L_threshold_cross - L_avg_inflect < int(1 * determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, True)):\n",
    "            L_avg_inflect = max(0, L_threshold_cross - int(1 * determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, True)))\n",
    "            left_max_slice = bmag[L_avg_inflect:L_threshold_cross + 1]\n",
    "            print(f\"Left peak search extended range: {L_avg_inflect} to {L_threshold_cross}, slice length: {len(left_max_slice)}\")\n",
    "        else:\n",
    "            left_max_slice = bmag[L_avg_inflect:L_threshold_cross + 1]\n",
    "            print(f\"Left peak search range: {L_avg_inflect} to {L_threshold_cross}, slice length: {len(left_max_slice)}\")\n",
    "\n",
    "        if len(left_max_slice) > 0:\n",
    "            left_max_value_idx = np.argmax(left_max_slice) + L_avg_inflect\n",
    "            left_max_value = bmag[left_max_value_idx]\n",
    "            print(f\"Left maximum detected at index {left_max_value_idx}, value: {left_max_value}\")\n",
    "        else:\n",
    "            print(\"Warning: Empty slice for finding the left maximum.\")\n",
    "            continue  # Skip this hole and move to the next one\n",
    "        #endregion\n",
    "        \n",
    "    #----- \u2705 5: Find the right peak (maximum) after the drop using the fast smoothing window-----//\n",
    "        #region Right Peak Scan Block\n",
    "        R_plateau_scan = R_threshold_cross\n",
    "        while R_plateau_scan < len(bmag_fast_smooth) - 1 and bmag_fast_smooth[R_plateau_scan] < bmag_fast_smooth[R_plateau_scan + 1]:\n",
    "            R_plateau_scan += 1\n",
    "        \n",
    "        R_avg_inflect = R_plateau_scan  # Assign the inflection point index for R\n",
    "        \n",
    "        # Now scan the region from R_avg_inflect to R_plateau_scan for the true maximum\n",
    "        slice_bmag = bmag[R_threshold_cross:R_avg_inflect + 1]\n",
    "        print(f\"Right peak search range: {R_threshold_cross} to {R_avg_inflect}, slice length: {len(slice_bmag)}\")\n",
    "        \n",
    "        if len(slice_bmag) > 0:\n",
    "            right_max_value_idx = np.argmax(slice_bmag) + R_threshold_cross\n",
    "            right_max_value_idx = min(right_max_value_idx, len(bmag) - 1)  # Ensure it's within bounds\n",
    "            right_max_value = bmag[right_max_value_idx]\n",
    "            print(f\"Right maximum detected at index {right_max_value_idx}, value: {right_max_value}\")\n",
    "        else:\n",
    "            # Handle the case where the slice is empty\n",
    "            print(f\"Warning: Empty slice for finding the right maximum: R_threshold_cross={R_threshold_cross}, R_avg_inflect={R_avg_inflect}\")\n",
    "            right_max_value_idx = min(R_threshold_cross, len(bmag) - 1)  # Ensure it's within bounds\n",
    "            right_max_value = bmag[right_max_value_idx]\n",
    "            break\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705 6: Process Asymmetry-----//\n",
    "        #region Assymetry Processing\n",
    "        hole_info = process_asymmetry(\n",
    "            left_max_value, right_max_value,\n",
    "            left_max_value_idx, right_max_value_idx,\n",
    "            L_threshold_cross, R_threshold_cross,\n",
    "            times_clipped, asymetric_peak_threshold,\n",
    "            symmetrical_peak_scan_window_in_secs,\n",
    "            bmag, bmag_slow_smooth, bmag_fast_smooth,\n",
    "            determine_sampling_rate, INSTRUMENT_SAMPLING_RATE,\n",
    "            max_window_seconds,\n",
    "            break_for_assymettry,\n",
    "            break_for_complex_hole\n",
    "        )\n",
    "    \n",
    "        if hole_info is None:  # If the processing was skipped due to asymmetry\n",
    "            hole_counter['asymmetric'] += 1\n",
    "            if break_for_assymettry:\n",
    "                continue  # Skip to the next iteration\n",
    "        else:\n",
    "            if hole_info.get(\"status\") == \"complex\":\n",
    "                hole_counter['complex'] += 1\n",
    "                continue  # Skip to the next iteration\n",
    "\n",
    "            if hole_info.get(\"status\") == \"unresolved_asymmetry\":\n",
    "                hole_counter['unresolved_asymmetry'] += 1\n",
    "                continue  # Skip to the next iteration\n",
    "\n",
    "            # If we're here, the hole is either not asymmetric or the asymmetry was resolved\n",
    "            if hole_info.get(\"asymmetrical_initial_peaks_flag\", False):\n",
    "                hole_counter['asymmetric_initial'] += 1\n",
    "\n",
    "            if hole_info.get(\"complex_hole_flag\", True):\n",
    "                hole_counter['complex_holes'] += 1\n",
    "\n",
    "            # Update all relevant variables based on the result from process_asymmetry  \n",
    "            left_max_value_idx = hole_info.get(\"left_max_value_idx\")\n",
    "            right_max_value_idx = hole_info.get(\"right_max_value_idx\")\n",
    "            left_max_value = hole_info.get(\"left_max_value\")\n",
    "            right_max_value = hole_info.get(\"right_max_value\")\n",
    "            L_threshold_cross = hole_info.get(\"L_threshold_cross\")\n",
    "            R_threshold_cross = hole_info.get(\"R_threshold_cross\")\n",
    "            min_idx = hole_info.get(\"min_idx\")\n",
    "            min_value = hole_info.get(\"min_value\")\n",
    "            complex_hole_flag = hole_info.get(\"complex_hole_flag\")\n",
    "        #endregion\n",
    "                \n",
    "    #----- \u2705 7: Determine the Bave for Bave_scan_seconds before and after the hole itself (save one value for before and one for after)-----//\n",
    "        #region Calculate Bave Block\n",
    "        # Calculate the sampling rate\n",
    "        sampling_rate = determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, use_calculated_sampling_rate)\n",
    "\n",
    "        # Convert search seconds into number of samples\n",
    "        half_second_samples = int(Bave_scan_seconds * sampling_rate)\n",
    "        \n",
    "        # Ensure the indices are within bounds\n",
    "        L_before_idx = max(0, left_max_value_idx - half_second_samples)\n",
    "        R_after_idx = min(len(bmag) - 1, right_max_value_idx + half_second_samples)\n",
    "        \n",
    "        # Calculate Bave before and after the hole\n",
    "        Bave_before = np.mean(bmag[L_before_idx:left_max_value_idx])\n",
    "        Bave_after = np.mean(bmag[right_max_value_idx:R_after_idx])\n",
    "        \n",
    "        print(f\"Bave_before: {Bave_before}, Bave_after: {Bave_after}\")\n",
    "        #endregion\n",
    "        \n",
    "    #----- \u2705 8: Calculate hole depth (take the avg of the two before and after values), first absolute hole depth and then relative-----//\n",
    "        #region Calculate Hole Depth Block\n",
    "        Bave = (Bave_before + Bave_after) / 2  # Average of the field before and after the hole\n",
    "        \n",
    "        # Calculate absolute hole depth\n",
    "        hole_abs_depth = Bave - min_value\n",
    "        # Calculate relative hole depth\n",
    "        hole_percentage_depth = (hole_abs_depth / Bave) * 100\n",
    "        \n",
    "        print(f\"Absolute Hole Depth: {hole_abs_depth}, Relative Hole Depth: {hole_percentage_depth}%\")\n",
    "        \n",
    "        # If the relative depth is not greater than depth_percentage_threshold, print a flag as \ud83c\uddf2\ud83c\udde6 too shallow\n",
    "        if hole_percentage_depth < depth_percentage_threshold * 100:  # Convert threshold to percentage for comparison\n",
    "            print(f\"\ud83c\uddf2\ud83c\udde6 Too shallow: relative depth is {hole_percentage_depth}%\")\n",
    "            hole_counter['shallow'] += 1\n",
    "            # Conditional behavior: If break_for_shallow_hole is enabled, skip this hole and move to the next\n",
    "            if break_for_shallow_hole:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this hole due to insufficient depth.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "        print(f\"-----\ud83d\udd73\ufe0f Hole relative depth is {hole_percentage_depth:.1f}%\")\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705 9: Calculate the hole change angle and redefine hole boundaries based on stdev-----//\n",
    "        #region Calculate Hole Angle and Boundaries Block\n",
    "        # Inside the main loop where you're processing holes:\n",
    "        tS, tE, W_angle = calculate_hole_angle_and_boundaries(\n",
    "            bmag, br, bt, bn, left_max_value_idx, right_max_value_idx, min_idx, \n",
    "            sampling_rate, Bave_window_seconds, wide_angle_threshold, break_for_wide_angle\n",
    "        )\n",
    "        \n",
    "        # If the hole was skipped, continue to the next iteration\n",
    "        if tS is None or tE is None or W_angle is None:\n",
    "            hole_counter['wide_angle'] += 1\n",
    "            if break_for_wide_angle:\n",
    "                continue\n",
    "        \n",
    "        # Update hole_info to include the new boundaries tS and tE\n",
    "        hole_info.update({\"tS\": tS, \"tE\": tE, \"W_angle\": W_angle})\n",
    "        #endregion\n",
    "    #----- \u2705 10: Calculate the number of First Derivative 0 Crossings (with a specified smoothing window) between beginning and end of the hole-----//\n",
    "        #region Zero Crossing Block \n",
    "        zero_crossings, zero_crossings_indices = analyze_derivative_zero_crossings(\n",
    "            bmag, times_clipped, left_max_value_idx, right_max_value_idx,\n",
    "            derivative_window_seconds, sampling_rate, mean_threshold,\n",
    "            OUTPUT_ZERO_CROSSING_PLOT\n",
    "        )\n",
    "\n",
    "        # Check if zero crossings exceed the threshold\n",
    "        if zero_crossings >= threshold_for_derivative_0_crossings_flag:\n",
    "            print(f\"\ud83c\uddf2\ud83c\udde6 Too many zero crossings: {zero_crossings} (Threshold: {threshold_for_derivative_0_crossings_flag})\")\n",
    "            hole_counter['derivative_crossings'] += 1\n",
    "            if break_for_derivative_crossings:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this hole due to excessive zero crossings.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "\n",
    "        # Update hole_info with zero crossings data\n",
    "        hole_info.update({\"zero_crossings\": zero_crossings})\n",
    "        #endregion  \n",
    "\n",
    "    #----- \u2705 11: Store the results-----//\n",
    "        #region Store Results Block\n",
    "        # Store the results and flags for the current hole\n",
    "        magnetic_hole_details.append(hole_info)\n",
    "        hole_minima.append(hole_info[\"min_idx\"])\n",
    "        hole_maxima_pairs.append((hole_info[\"left_max_value_idx\"], hole_info[\"right_max_value_idx\"]))\n",
    "        magnetic_holes.append((hole_info[\"L_threshold_cross\"], hole_info[\"R_threshold_cross\"]))\n",
    "        \n",
    "\n",
    "        if search_in_progress_output:\n",
    "            print(f\"-----\u2b50\ufe0f Magnetic hole identified from index {hole_info['left_max_value_idx']} to {hole_info['right_max_value_idx']}\")\n",
    "                \n",
    "        i = hole_info[\"R_threshold_cross\"] + 1\n",
    "        hole_counter['confirmed'] += 1\n",
    "        #endregion\n",
    "    \n",
    "    print(f\"Returning: {len(magnetic_holes)} holes, {len(hole_minima)} minima, {len(hole_maxima_pairs)} max pairs\")\n",
    "    return magnetic_holes, hole_minima, hole_maxima_pairs, times_clipped, bmag,  magnetic_hole_details\n",
    "\n",
    "#-------- \u2699\ufe0f Constants--------//\n",
    "#region Constants Block\n",
    "INSTRUMENT_SAMPLING_RATE = 292.9  # Will not be used if use_calculated_sampling_rate = 1\n",
    "use_calculated_sampling_rate = 1\n",
    "depth_percentage_threshold = .25\n",
    "smoothing_window_seconds = 8\n",
    "derivative_window_seconds = .2\n",
    "min_max_finding_smooth_window = .3\n",
    "mean_threshold = 0.8\n",
    "search_in_progress_output = True\n",
    "additional_seconds_for_min_search = 0.2\n",
    "asymetric_peak_threshold = .25\n",
    "symmetrical_peak_scan_window_in_secs = 2\n",
    "Bave_scan_seconds = .1\n",
    "Bave_window_seconds = 20\n",
    "wide_angle_threshold = 15\n",
    "small_threshold_cross_flag = 10\n",
    "small_threshold_cross_adjustment = 10\n",
    "#endregion\n",
    "\n",
    "smoothing_window_seconds = 8  # 8-second smoothing for primary detection\n",
    "derivative_window_seconds = .2  # Window for smoothing before calculating the first derivative\n",
    "\n",
    "# min_max_finding_smooth_window = .05  # 0.05 was being tried for a special case and seemed to work well...\n",
    "min_max_finding_smooth_window = .3  # 0.3-second smoothing for detailed max finding, this was the OG\n",
    "\n",
    "mean_threshold = 0.8  # 80% of the smoothed value\n",
    "search_in_progress_output = True  # Set to True for detailed output\n",
    "additional_seconds_for_min_search = 0.2  # Extend the search range\n",
    "\n",
    "# Define a threshold for peak asymmetry\n",
    "asymetric_peak_threshold = .25  # \ud83e\ude90 25% asymmetry threshold, was originally 10\n",
    "symmetrical_peak_scan_window_in_secs = 2  # Time window to scan for symmetrical peaks in seconds\n",
    "\n",
    "Bave_scan_seconds = .1\n",
    "\n",
    "# New constant for wide angle threshold\n",
    "Bave_window_seconds = 20  # Window size for calculating Bave and delta_B, this is where the angle is calculated\n",
    "wide_angle_threshold = 15  # 15 degrees threshold for W angle\n",
    "\n",
    "small_threshold_cross_flag = 10  # Define the small threshold cross (i.e. we only cross the threshold for _ samples)\n",
    "small_threshold_cross_adjustment = 10  # Define the adjustment when small threshold cross is detected (i.e. widen by _ to help our algorithm)\n",
    "#endregion\n",
    "\n",
    "# -------- \ud83e\udd17 File Output & Plotting Options--------//\n",
    "#region File Output & Plotting Options Block\n",
    "OUTPUT_PLOT = 1  # Set to 1 to output the plot\n",
    "plot_hole_minimum = 0  # Set to 1 to plot green lines at the minimum points within holes\n",
    "plot_smooth_threshold_crossing = 1  # Set to 1 to plot blue and purple lines at smooth threshold crossings (see mean_threshold defined below)\n",
    "SAVE_PLOT = 1 #Save the plot to the subdirectory\n",
    "\n",
    "\n",
    "OUTPUT_ZERO_CROSSING_PLOT = 0 #\u26f0\ufe0f\n",
    "\n",
    "IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN = 1  # Set to 1 to save iZotope formatted output with max and min indices to a .txt file\n",
    "MARKER_FILE_VERSION = 3  # Version number for marker file\n",
    "IZOTOPE_MARKER_FILE_OUTPUT = 0\n",
    "\n",
    "EXPORT_AUDIO_FILES = 1  # Set to 1 to export audio files\n",
    "AUDIO_SAMPLING_RATE = 22000  # Sampling rate for the audio files\n",
    "\n",
    "# New Option for Annotated Markers\n",
    "Marker_Files_With_Annotated_Markers = 0  # Set to 1 to include drop percentage and width in marker name\n",
    "Marker_Files_With_Hole_Numbers = 0\n",
    "\n",
    "SEARCH_IN_PROGRESS_OUTPUT = 1\n",
    "\n",
    "#-------- \ud83d\udcc8 Plotting Options --------//\n",
    "plot_hole_minimum = 1  # 1 to plot minima, 0 to skip\n",
    "plot_smooth_threshold_crossing = 1  # 1 to plot threshold crossings, 0 to skip\n",
    "#endregion\n",
    "\n",
    "#-------- \ud83c\uddf2\ud83c\udde6 Flag Handling--------//\n",
    "#region Flag Handling Block \n",
    "break_for_shallow_hole = 1 # \ud83c\uddf2\ud83c\udde6 flag for skipping if a hole is too shallow\n",
    "break_for_assymettry = 0 # \ud83c\uddf2\ud83c\udde6 flag for skipping if a whole is assymetrical\n",
    "break_for_wide_angle = 0  # \ud83c\uddf2\ud83c\udde6 flag for handling wide angle cases\n",
    "break_for_small_threshold_cross = 0  # \ud83c\uddf2\ud83c\udde6 flag to skip small threshold crosses\n",
    "break_for_complex_hole = 0 # \ud83c\uddf2\ud83c\udde6 flag for skipping complex holes\n",
    "threshold_for_derivative_0_crossings_flag = 1000 #If the .1s smoothed first derivative crosses this many times or higher, raise a flag\n",
    "break_for_derivative_crossings = 0\n",
    "#endregion\n",
    "\n",
    "#--------\u23f1\ufe0f Set time range--------//\n",
    "#region Time Range Block\n",
    "# Define the input parameters\n",
    "trange = ['2023-09-28/06:10:00.000', '2023-09-28/07:10:00.000'] #Whole Shebang\n",
    "trange = ['2023-09-28/06:10:00.000', '2023-09-28/07:10:00.000'] #Whole Shebang\n",
    "# trange = ['2023-09-28/06:37:15.000', '2023-09-28/06:37:45.000'] #wide lots o dips\n",
    "# trange = ['2023-09-28/06:37:05.500', '2023-09-28/06:37:08.000'] #Interesting Test Case WE'VE BEEN USING for Derivative\n",
    "# trange = ['2023-09-28/06:39:00.000', '2023-09-28/06:40:10.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f GREAT region for solid hole testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:38:10.000', '2023-09-28/06:40:10.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f GREAT region for solid hole testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:39:07.000', '2023-09-28/06:39:10.000'] #WONKY WONKS\n",
    "# trange = ['2023-09-28/06:39:50.000', '2023-09-28/06:39:55.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f Checking a single hole\n",
    "# trange = ['2023-09-28/06:39:31.000', '2023-09-28/06:39:35.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f PERFECT Single hole for testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:39:45.000', '2023-09-28/06:39:50.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f ANOTHER PERFECT Single hole for testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:37:05.500', '2023-09-28/06:37:08.000'] #Interesting Test Case WE'VE BEEN USING for Derivative\n",
    "# trange = ['2023-09-28/06:36:022', '2023-09-28/06:36:24'] # A pretty hole \u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:29:18', '2023-09-28/06:29:22'] #with a larger window #\ud83c\uddf2\ud83c\udde6 Asymmetry between peaks: 42.25%\n",
    "#endregion\n",
    "\n",
    "time_check(trange)\n",
    "\n",
    "# At the beginning of your main script, after setting up save_dir\n",
    "sub_save_dir = setup_output_directory(trange, save_dir)\n",
    "print(f\"\ud83d\ude4c\ud83d\ude4c\ud83d\ude4cMain script: sub_save_dir set to {sub_save_dir}\")\n",
    "\n",
    "# Call the function to detect magnetic holes\n",
    "magnetic_holes, hole_minima, hole_maxima_pairs, times_clipped, bmag, magnetic_hole_details = detect_magnetic_holes(\n",
    "    trange, \n",
    "    smoothing_window_seconds, \n",
    "    min_max_finding_smooth_window, \n",
    "    mean_threshold, \n",
    "    search_in_progress_output, \n",
    "    Bave_scan_seconds, \n",
    "    break_for_assymettry, \n",
    "    small_threshold_cross_flag,          \n",
    "    small_threshold_cross_adjustment,    \n",
    "    break_for_small_threshold_cross,     \n",
    "    break_for_complex_hole,\n",
    "    derivative_window_seconds,  # Pass the derivative window into the function\n",
    "    OUTPUT_ZERO_CROSSING_PLOT,\n",
    "    SEARCH_IN_PROGRESS_OUTPUT,\n",
    "    INSTRUMENT_SAMPLING_RATE,\n",
    "    use_calculated_sampling_rate\n",
    "    \n",
    ")\n",
    "\n",
    "# Plotting the results if configured\n",
    "if OUTPUT_PLOT:\n",
    "    plot_mag_data_with_holes_and_minimum(\n",
    "        times_clipped, \n",
    "        bmag, \n",
    "        magnetic_holes, \n",
    "        hole_minima, \n",
    "        hole_maxima_pairs, \n",
    "        plot_hole_minimum, \n",
    "        plot_smooth_threshold_crossing,\n",
    "        trange,  # Add this\n",
    "        save_dir,  # Add this\n",
    "        SAVE_PLOT  # Add this if you don't want to save the plot by default\n",
    "    )\n",
    "# Create the settings dictionary\n",
    "settings = {\n",
    "    \"INSTRUMENT_SAMPLING_RATE\": INSTRUMENT_SAMPLING_RATE,\n",
    "    \"use_calculated_sampling_rate\": use_calculated_sampling_rate,\n",
    "    \"depth_percentage_threshold\": depth_percentage_threshold,\n",
    "    \"smoothing_window_seconds\": smoothing_window_seconds,\n",
    "    \"derivative_window_seconds\": derivative_window_seconds,\n",
    "    \"min_max_finding_smooth_window\": min_max_finding_smooth_window,\n",
    "    \"mean_threshold\": mean_threshold,\n",
    "    \"search_in_progress_output\": search_in_progress_output,\n",
    "    \"additional_seconds_for_min_search\": additional_seconds_for_min_search,\n",
    "    \"asymetric_peak_threshold\": asymetric_peak_threshold,\n",
    "    \"symmetrical_peak_scan_window_in_secs\": symmetrical_peak_scan_window_in_secs,\n",
    "    \"Bave_scan_seconds\": Bave_scan_seconds,\n",
    "    \"Bave_window_seconds\": Bave_window_seconds,\n",
    "    \"wide_angle_threshold\": wide_angle_threshold,\n",
    "    \"small_threshold_cross_flag\": small_threshold_cross_flag,\n",
    "    \"small_threshold_cross_adjustment\": small_threshold_cross_adjustment,\n",
    "    \"OUTPUT_PLOT\": OUTPUT_PLOT,\n",
    "    \"plot_hole_minimum\": plot_hole_minimum,\n",
    "    \"plot_smooth_threshold_crossing\": plot_smooth_threshold_crossing,\n",
    "    \"OUTPUT_ZERO_CROSSING_PLOT\": OUTPUT_ZERO_CROSSING_PLOT,\n",
    "    \"IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN\": IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN,\n",
    "    \"MARKER_FILE_VERSION\": MARKER_FILE_VERSION,\n",
    "    \"IZOTOPE_MARKER_FILE_OUTPUT\": IZOTOPE_MARKER_FILE_OUTPUT,\n",
    "    \"EXPORT_AUDIO_FILES\": EXPORT_AUDIO_FILES,\n",
    "    \"AUDIO_SAMPLING_RATE\": AUDIO_SAMPLING_RATE,\n",
    "    \"Marker_Files_With_Annotated_Markers\": Marker_Files_With_Annotated_Markers,\n",
    "    \"SEARCH_IN_PROGRESS_OUTPUT\": SEARCH_IN_PROGRESS_OUTPUT,\n",
    "    \"break_for_shallow_hole\": break_for_shallow_hole,\n",
    "    \"break_for_assymettry\": break_for_assymettry,\n",
    "    \"break_for_wide_angle\": break_for_wide_angle,\n",
    "    \"break_for_small_threshold_cross\": break_for_small_threshold_cross,\n",
    "    \"break_for_complex_hole\": break_for_complex_hole,\n",
    "    \"threshold_for_derivative_0_crossings_flag\": threshold_for_derivative_0_crossings_flag,\n",
    "    \"break_for_derivative_crossings\": break_for_derivative_crossings\n",
    "}\n",
    "\n",
    "# Get the sub-directory path\n",
    "# sub_save_dir = setup_output_directory(trange, save_dir)\n",
    "\n",
    "# last_dir_file = \"last_selected_dir.txt\"\n",
    "# save_dir = set_save_directory(last_dir_file)\n",
    "print(f'\ud83d\udedf save_dir = {save_dir}')\n",
    "\n",
    "# Use sub_save_dir for all file operations\n",
    "settings_file_path = os.path.join(sub_save_dir, 'magnetic_hole_detection_settings.json')\n",
    "with open(settings_file_path, 'w') as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "print(f\"Settings saved to: {settings_file_path}\")\n",
    "\n",
    "# Generate marker file if needed\n",
    "if IZOTOPE_MARKER_FILE_OUTPUT or IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN:\n",
    "    # Call the output_magnetic_holes function with the settings dictionary\n",
    "    output_magnetic_holes(\n",
    "        magnetic_holes,\n",
    "        hole_maxima_pairs,\n",
    "        times_clipped,\n",
    "        bmag,\n",
    "        IZOTOPE_MARKER_FILE_OUTPUT,\n",
    "        IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN,\n",
    "        trange,\n",
    "        MARKER_FILE_VERSION,\n",
    "        SEARCH_IN_PROGRESS_OUTPUT,\n",
    "        save_dir,\n",
    "        INSTRUMENT_SAMPLING_RATE,\n",
    "        Marker_Files_With_Annotated_Markers,\n",
    "        Marker_Files_With_Hole_Numbers,\n",
    "        magnetic_hole_details\n",
    "    )\n",
    "\n",
    "# Export audio files if needed\n",
    "if EXPORT_AUDIO_FILES:\n",
    "    audify_high_res_mag_data_without_plot(trange[0], trange[1], sub_save_dir, AUDIO_SAMPLING_RATE, sub_save_dir)\n",
    "\n",
    "# Summary print statements\n",
    "print(\"\\n--- Magnetic Hole Detection Summary ---\")\n",
    "print(f\"\ud83d\udd22 {hole_counter['potential']} Total potential holes identified\")\n",
    "print(f\"\u2705 {hole_counter['confirmed']} Total holes confirmed\")\n",
    "print(f\"\ud83c\udf00 {hole_counter['complex_holes']} Complex/Assymetrical holes\")\n",
    "print(f\"{'\u26d4\ufe0f' if break_for_shallow_hole else '\ud83c\udd97'} {hole_counter['shallow']} Shallow holes\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_assymettry else '\ud83c\udd97'} {hole_counter['asymmetric']} Asymmetric holes\")\n",
    "print(f\"{'\u26d4\ufe0f' if break_for_wide_angle else '\ud83c\udd97'} {hole_counter['wide_angle']} Wide angle holes\")\n",
    "print(f\"{'\u26d4\ufe0f' if break_for_small_threshold_cross else '\ud83c\udd97'} {hole_counter['small_threshold_cross']} Small threshold crosses\")\n",
    "print(f\"{'\u26d4\ufe0f' if break_for_assymettry else '\ud83c\udd97'} {hole_counter['unresolved_asymmetry']} Unresolved asymmetric holes\")\n",
    "print(f\"\ud83c\udd97 {hole_counter['asymmetric_initial']} Initially asymmetric holes (resolved)\")\n",
    "# print(f\"\ud83c\udd97 {hole_counter['complex_resolved']} Complex holes (resolved)\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_complex_hole else '\ud83c\udd97'} {hole_counter.get('complex', 0)} Complex holes\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_derivative_crossings else '\ud83c\udd97'} {hole_counter['derivative_crossings']} Excessive derivative crossings\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "print(\"Settings have been saved to a JSON file in the sub-directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d386d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR SAVING AUDIO **NOT WORKING**\n",
    "global save_dir\n",
    "\n",
    "import importlib\n",
    "\n",
    "#region Import Modules Block\n",
    "from collections import Counter\n",
    "import asymmetry_calc\n",
    "importlib.reload(asymmetry_calc)\n",
    "from asymmetry_calc import *\n",
    "import time_management\n",
    "importlib.reload(time_management)\n",
    "from time_management import *\n",
    "import data_management\n",
    "importlib.reload(data_management)\n",
    "from data_management import *\n",
    "import plotting\n",
    "importlib.reload(plotting)\n",
    "from plotting import *\n",
    "import data_management\n",
    "importlib.reload(data_management)\n",
    "from data_management import *\n",
    "import data_audification\n",
    "importlib.reload(data_audification)\n",
    "from data_audification import *\n",
    "import plotting\n",
    "importlib.reload(plotting)\n",
    "from plotting import *\n",
    "import zero_crossing_analysis\n",
    "importlib.reload(zero_crossing_analysis)\n",
    "from zero_crossing_analysis import analyze_derivative_zero_crossings\n",
    "import MH_format_output\n",
    "importlib.reload(MH_format_output)\n",
    "from MH_format_output import *\n",
    "\n",
    "import hole_angle_calc\n",
    "importlib.reload(hole_angle_calc)\n",
    "from hole_angle_calc import *\n",
    "\n",
    "\n",
    "# from hole_counter import hole_counter\n",
    "#endregion  \n",
    "\n",
    "# Initialize the counter\n",
    "hole_counter = Counter()\n",
    "\n",
    "def detect_magnetic_holes(trange, smoothing_window_seconds, min_max_finding_smooth_window, mean_threshold, search_in_progress_output, Bave_scan_seconds, break_for_assymettry, small_threshold_cross_flag, small_threshold_cross_adjustment, break_for_small_threshold_cross, break_for_complex_hole, derivative_window_seconds, OUTPUT_ZERO_CROSSING_PLOT, SEARCH_IN_PROGRESS_OUTPUT, INSTRUMENT_SAMPLING_RATE, use_calculated_sampling_rate):\n",
    "    global hole_counter  # Add this line to use the global hole_counter\n",
    "    \n",
    "    max_window_seconds = smoothing_window_seconds\n",
    "    \n",
    "    # \ud83d\udcc8 Data Preparation Step 1: Extend the Time Range\n",
    "    extended_trange = extend_time_range(trange, max(smoothing_window_seconds, min_max_finding_smooth_window))\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 2: Download Data for the Extended Range\n",
    "    times, br, bt, bn, bmag_extended = download_and_prepare_high_res_mag_data(extended_trange)\n",
    "    print(f\"length of bmag_extended is {len(bmag_extended)}\")\n",
    "\n",
    "    # Calculate sampling rate based on the extended data\n",
    "    total_samples = len(bmag_extended)\n",
    "    start_time = datetime.strptime(extended_trange[0], '%Y-%m-%d/%H:%M:%S.%f' if '.' in extended_trange[0] else '%Y-%m-%d/%H:%M:%S')\n",
    "    end_time = datetime.strptime(extended_trange[1], '%Y-%m-%d/%H:%M:%S.%f' if '.' in extended_trange[1] else '%Y-%m-%d/%H:%M:%S')\n",
    "    duration = end_time - start_time\n",
    "    duration_seconds = duration.total_seconds()\n",
    "\n",
    "    if use_calculated_sampling_rate:\n",
    "        INSTRUMENT_SAMPLING_RATE = total_samples / duration_seconds\n",
    "        print(f\"\u2733\ufe0f Using calculated SR of {INSTRUMENT_SAMPLING_RATE:.2f} Hz\")\n",
    "    else:\n",
    "        print(f\"\u2733\ufe0f Using predefined SR of {INSTRUMENT_SAMPLING_RATE} Hz\")\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 3: Apply smoothing to the bmag data for detection and maxima finding\n",
    "    bmag_slow_smooth_extended = efficient_moving_average(times, bmag_extended, smoothing_window_seconds, determine_sampling_rate(times, INSTRUMENT_SAMPLING_RATE, True), mean_threshold)\n",
    "    bmag_fast_smooth_extended = efficient_moving_average(times, bmag_extended, min_max_finding_smooth_window, determine_sampling_rate(times, INSTRUMENT_SAMPLING_RATE, True), mean_threshold)\n",
    "\n",
    "    # \ud83d\udcc8 Data Preparation Step 4: Clip the smoothed data to the original time range\n",
    "    times_clipped, bmag = clip_to_original_time_range(times, bmag_extended, trange)\n",
    "    _, bmag_slow_smooth = clip_to_original_time_range(times, bmag_slow_smooth_extended, trange)\n",
    "    _, bmag_fast_smooth = clip_to_original_time_range(times, bmag_fast_smooth_extended, trange)\n",
    "\n",
    "    print(f\"Post-clipping length of bmag is {len(bmag)}\")\n",
    "\n",
    "    # # Calculate sampling rate\n",
    "    # total_samples = len(bmag)\n",
    "    # start_time = datetime.strptime(trange[0], '%Y-%m-%d/%H:%M:%S.%f' if '.' in trange[0] else '%Y-%m-%d/%H:%M:%S')\n",
    "    # end_time = datetime.strptime(trange[1], '%Y-%m-%d/%H:%M:%S.%f' if '.' in trange[1] else '%Y-%m-%d/%H:%M:%S')\n",
    "    # duration = end_time - start_time\n",
    "    # duration_seconds = duration.total_seconds()\n",
    "\n",
    "    # if USE_CALCULATED_SAMPLING_RATE:\n",
    "    #     calculated_sampling_rate = total_samples / duration_seconds\n",
    "    #     INSTRUMENT_SAMPLING_RATE = calculated_sampling_rate\n",
    "    #     print(f\"\u2733\ufe0f Using calculated SR of {INSTRUMENT_SAMPLING_RATE:.2f} Hz\")\n",
    "    # else:\n",
    "    #     print(f\"\u2733\ufe0f Using predefined SR of {INSTRUMENT_SAMPLING_RATE} Hz\")\n",
    "\n",
    "    \n",
    "    magnetic_holes = []\n",
    "    hole_minima = []  # To store the minima for plotting\n",
    "    hole_maxima_pairs = []  # To store the left and right maxima pairs for plotting\n",
    "    magnetic_hole_details = []  # To store details of each detected magnetic hole\n",
    "    i = 0\n",
    "\n",
    "    #----- \u2705\u2705\u2705 1: Scan through the data to detect drops below the slow smoothed value-----//\n",
    "    \n",
    "    while i < len(bmag):   \n",
    "        while i < len(bmag) and bmag[i] > bmag_slow_smooth[i]:  # skip over where bmag > bmag_slow_smooth\n",
    "            i += 1\n",
    "        if i >= len(bmag):\n",
    "            break    \n",
    "        L_threshold_cross = i\n",
    "        hole_counter['potential'] += 1\n",
    "        \n",
    "    #----- \u2705\u2705\u2705 2: Continue scanning forward until bmag rises above the slow smoothed value-----// \n",
    "        #region Right Threshold Cross Block\n",
    "        while i < len(bmag) - 1 and bmag[i] < bmag_slow_smooth[i]:\n",
    "            i += 1\n",
    "        R_threshold_cross = i\n",
    "\n",
    "        if R_threshold_cross - L_threshold_cross <= small_threshold_cross_flag:\n",
    "            hole_counter['small_threshold_cross'] += 1\n",
    "            if break_for_small_threshold_cross:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this small threshold cross.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "            else:\n",
    "                R_threshold_cross += small_threshold_cross_adjustment\n",
    "                L_threshold_cross -= small_threshold_cross_adjustment\n",
    "                print(f\"\ud83d\udd25 Hole detection threshold crossed near the bottom of a hole, expanding L & R thresholds for analysis:\")\n",
    "                print(f\"New L_threshold_cross: {L_threshold_cross}\")\n",
    "                print(f\"New R_threshold_cross: {R_threshold_cross}\")\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705\u2705\u2705 3: Scan for minimum where bmag[i] < slow smoothed value-----//\n",
    "        min_idx = np.argmin(bmag[L_threshold_cross:R_threshold_cross + 1]) + L_threshold_cross  # Offset to slice start, end is exclusive so +1\n",
    "        min_value = bmag[min_idx]\n",
    "        print(f\"Minimum initially identified at {min_idx}\")\n",
    "    \n",
    "    #----- \u2705 4: Find the left peak (maximum) before the drop using the fast smoothing window-----//\n",
    "        #region Left Peak Scan Block\n",
    "        # Move left while the current smoothed value is lower than the previous smoothed value\n",
    "        L_plateau_scan = L_threshold_cross\n",
    "        while L_plateau_scan > 0 and bmag_fast_smooth[L_plateau_scan] < bmag_fast_smooth[L_plateau_scan - 1]:\n",
    "            L_plateau_scan -= 1\n",
    "        L_avg_inflect = L_plateau_scan\n",
    "\n",
    "        # If the identified inflection point is too close to the threshold cross, scan further back in time\n",
    "        if L_threshold_cross - L_avg_inflect < int(1 * determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, True)):\n",
    "            L_avg_inflect = max(0, L_threshold_cross - int(1 * determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, True)))\n",
    "            left_max_slice = bmag[L_avg_inflect:L_threshold_cross + 1]\n",
    "            print(f\"Left peak search extended range: {L_avg_inflect} to {L_threshold_cross}, slice length: {len(left_max_slice)}\")\n",
    "        else:\n",
    "            left_max_slice = bmag[L_avg_inflect:L_threshold_cross + 1]\n",
    "            print(f\"Left peak search range: {L_avg_inflect} to {L_threshold_cross}, slice length: {len(left_max_slice)}\")\n",
    "\n",
    "        if len(left_max_slice) > 0:\n",
    "            left_max_value_idx = np.argmax(left_max_slice) + L_avg_inflect\n",
    "            left_max_value = bmag[left_max_value_idx]\n",
    "            print(f\"Left maximum detected at index {left_max_value_idx}, value: {left_max_value}\")\n",
    "        else:\n",
    "            print(\"Warning: Empty slice for finding the left maximum.\")\n",
    "            continue  # Skip this hole and move to the next one\n",
    "        #endregion\n",
    "        \n",
    "    #----- \u2705 5: Find the right peak (maximum) after the drop using the fast smoothing window-----//\n",
    "        #region Right Peak Scan Block\n",
    "        R_plateau_scan = R_threshold_cross\n",
    "        while R_plateau_scan < len(bmag_fast_smooth) - 1 and bmag_fast_smooth[R_plateau_scan] < bmag_fast_smooth[R_plateau_scan + 1]:\n",
    "            R_plateau_scan += 1\n",
    "        \n",
    "        R_avg_inflect = R_plateau_scan  # Assign the inflection point index for R\n",
    "        \n",
    "        # Now scan the region from R_avg_inflect to R_plateau_scan for the true maximum\n",
    "        slice_bmag = bmag[R_threshold_cross:R_avg_inflect + 1]\n",
    "        print(f\"Right peak search range: {R_threshold_cross} to {R_avg_inflect}, slice length: {len(slice_bmag)}\")\n",
    "        \n",
    "        if len(slice_bmag) > 0:\n",
    "            right_max_value_idx = np.argmax(slice_bmag) + R_threshold_cross\n",
    "            right_max_value_idx = min(right_max_value_idx, len(bmag) - 1)  # Ensure it's within bounds\n",
    "            right_max_value = bmag[right_max_value_idx]\n",
    "            print(f\"Right maximum detected at index {right_max_value_idx}, value: {right_max_value}\")\n",
    "        else:\n",
    "            # Handle the case where the slice is empty\n",
    "            print(f\"Warning: Empty slice for finding the right maximum: R_threshold_cross={R_threshold_cross}, R_avg_inflect={R_avg_inflect}\")\n",
    "            right_max_value_idx = min(R_threshold_cross, len(bmag) - 1)  # Ensure it's within bounds\n",
    "            right_max_value = bmag[right_max_value_idx]\n",
    "            break\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705 6: Process Asymmetry-----//\n",
    "        #region Assymetry Processing\n",
    "        hole_info = process_asymmetry(\n",
    "            left_max_value, right_max_value,\n",
    "            left_max_value_idx, right_max_value_idx,\n",
    "            L_threshold_cross, R_threshold_cross,\n",
    "            times_clipped, asymetric_peak_threshold,\n",
    "            symmetrical_peak_scan_window_in_secs,\n",
    "            bmag, bmag_slow_smooth, bmag_fast_smooth,\n",
    "            determine_sampling_rate, INSTRUMENT_SAMPLING_RATE,\n",
    "            max_window_seconds,\n",
    "            break_for_assymettry,\n",
    "            break_for_complex_hole\n",
    "        )\n",
    "    \n",
    "        if hole_info is None:  # If the processing was skipped due to asymmetry\n",
    "            hole_counter['asymmetric'] += 1\n",
    "            if break_for_assymettry:\n",
    "                continue  # Skip to the next iteration\n",
    "        else:\n",
    "            if hole_info.get(\"status\") == \"complex\":\n",
    "                hole_counter['complex'] += 1\n",
    "                continue  # Skip to the next iteration\n",
    "\n",
    "            if hole_info.get(\"status\") == \"unresolved_asymmetry\":\n",
    "                hole_counter['unresolved_asymmetry'] += 1\n",
    "                continue  # Skip to the next iteration\n",
    "\n",
    "            # If we're here, the hole is either not asymmetric or the asymmetry was resolved\n",
    "            if hole_info.get(\"asymmetrical_initial_peaks_flag\", False):\n",
    "                hole_counter['asymmetric_initial'] += 1\n",
    "\n",
    "            if hole_info.get(\"complex_hole_flag\", True):\n",
    "                hole_counter['complex_holes'] += 1\n",
    "\n",
    "            # Update all relevant variables based on the result from process_asymmetry  \n",
    "            left_max_value_idx = hole_info.get(\"left_max_value_idx\")\n",
    "            right_max_value_idx = hole_info.get(\"right_max_value_idx\")\n",
    "            left_max_value = hole_info.get(\"left_max_value\")\n",
    "            right_max_value = hole_info.get(\"right_max_value\")\n",
    "            L_threshold_cross = hole_info.get(\"L_threshold_cross\")\n",
    "            R_threshold_cross = hole_info.get(\"R_threshold_cross\")\n",
    "            min_idx = hole_info.get(\"min_idx\")\n",
    "            min_value = hole_info.get(\"min_value\")\n",
    "            complex_hole_flag = hole_info.get(\"complex_hole_flag\")\n",
    "        #endregion\n",
    "                \n",
    "    #----- \u2705 7: Determine the Bave for Bave_scan_seconds before and after the hole itself (save one value for before and one for after)-----//\n",
    "        #region Calculate Bave Block\n",
    "        # Calculate the sampling rate\n",
    "        sampling_rate = determine_sampling_rate(times_clipped, INSTRUMENT_SAMPLING_RATE, use_calculated_sampling_rate)\n",
    "\n",
    "        # Convert search seconds into number of samples\n",
    "        half_second_samples = int(Bave_scan_seconds * sampling_rate)\n",
    "        \n",
    "        # Ensure the indices are within bounds\n",
    "        L_before_idx = max(0, left_max_value_idx - half_second_samples)\n",
    "        R_after_idx = min(len(bmag) - 1, right_max_value_idx + half_second_samples)\n",
    "        \n",
    "        # Calculate Bave before and after the hole\n",
    "        Bave_before = np.mean(bmag[L_before_idx:left_max_value_idx])\n",
    "        Bave_after = np.mean(bmag[right_max_value_idx:R_after_idx])\n",
    "        \n",
    "        print(f\"Bave_before: {Bave_before}, Bave_after: {Bave_after}\")\n",
    "        #endregion\n",
    "        \n",
    "    #----- \u2705 8: Calculate hole depth (take the avg of the two before and after values), first absolute hole depth and then relative-----//\n",
    "        #region Calculate Hole Depth Block\n",
    "        Bave = (Bave_before + Bave_after) / 2  # Average of the field before and after the hole\n",
    "        \n",
    "        # Calculate absolute hole depth\n",
    "        hole_abs_depth = Bave - min_value\n",
    "        # Calculate relative hole depth\n",
    "        hole_percentage_depth = (hole_abs_depth / Bave) * 100\n",
    "        \n",
    "        print(f\"Absolute Hole Depth: {hole_abs_depth}, Relative Hole Depth: {hole_percentage_depth}%\")\n",
    "        \n",
    "        # If the relative depth is not greater than depth_percentage_threshold, print a flag as \ud83c\uddf2\ud83c\udde6 too shallow\n",
    "        if hole_percentage_depth < depth_percentage_threshold * 100:  # Convert threshold to percentage for comparison\n",
    "            print(f\"\ud83c\uddf2\ud83c\udde6 Too shallow: relative depth is {hole_percentage_depth}%\")\n",
    "            hole_counter['shallow'] += 1\n",
    "            # Conditional behavior: If break_for_shallow_hole is enabled, skip this hole and move to the next\n",
    "            if break_for_shallow_hole:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this hole due to insufficient depth.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "        print(f\"-----\ud83d\udd73\ufe0f Hole relative depth is {hole_percentage_depth:.1f}%\")\n",
    "        #endregion\n",
    "\n",
    "    #----- \u2705 9: Calculate the hole change angle and redefine hole boundaries based on stdev-----//\n",
    "        #region Calculate Hole Angle and Boundaries Block\n",
    "        # Inside the main loop where you're processing holes:\n",
    "        tS, tE, W_angle = calculate_hole_angle_and_boundaries(\n",
    "            bmag, br, bt, bn, left_max_value_idx, right_max_value_idx, min_idx, \n",
    "            sampling_rate, Bave_window_seconds, wide_angle_threshold, break_for_wide_angle\n",
    "        )\n",
    "        \n",
    "        # If the hole was skipped, continue to the next iteration\n",
    "        if tS is None or tE is None or W_angle is None:\n",
    "            hole_counter['wide_angle'] += 1\n",
    "            if break_for_wide_angle:\n",
    "                continue\n",
    "        \n",
    "        # Update hole_info to include the new boundaries tS and tE\n",
    "        hole_info.update({\"tS\": tS, \"tE\": tE, \"W_angle\": W_angle})\n",
    "        #endregion\n",
    "    #----- \u2705 10: Calculate the number of First Derivative 0 Crossings (with a specified smoothing window) between beginning and end of the hole-----//\n",
    "        #region Zero Crossing Block \n",
    "        zero_crossings, zero_crossings_indices = analyze_derivative_zero_crossings(\n",
    "            bmag, times_clipped, left_max_value_idx, right_max_value_idx,\n",
    "            derivative_window_seconds, sampling_rate, mean_threshold,\n",
    "            OUTPUT_ZERO_CROSSING_PLOT\n",
    "        )\n",
    "\n",
    "        # Check if zero crossings exceed the threshold\n",
    "        if zero_crossings >= threshold_for_derivative_0_crossings_flag:\n",
    "            print(f\"\ud83c\uddf2\ud83c\udde6 Too many zero crossings: {zero_crossings} (Threshold: {threshold_for_derivative_0_crossings_flag})\")\n",
    "            hole_counter['derivative_crossings'] += 1\n",
    "            if break_for_derivative_crossings:\n",
    "                print(\"-----\u26d4\ufe0f Skipping this hole due to excessive zero crossings.\")\n",
    "                continue  # Skip the remaining processing for this hole and move to the next iteration\n",
    "\n",
    "        # Update hole_info with zero crossings data\n",
    "        hole_info.update({\"zero_crossings\": zero_crossings})\n",
    "        #endregion  \n",
    "\n",
    "    #----- \u2705 11: Store the results-----//\n",
    "        #region Store Results Block\n",
    "        # Store the results and flags for the current hole\n",
    "        magnetic_hole_details.append(hole_info)\n",
    "        hole_minima.append(hole_info[\"min_idx\"])\n",
    "        hole_maxima_pairs.append((hole_info[\"left_max_value_idx\"], hole_info[\"right_max_value_idx\"]))\n",
    "        magnetic_holes.append((hole_info[\"L_threshold_cross\"], hole_info[\"R_threshold_cross\"]))\n",
    "        \n",
    "\n",
    "        if search_in_progress_output:\n",
    "            print(f\"-----\u2b50\ufe0f Magnetic hole identified from index {hole_info['left_max_value_idx']} to {hole_info['right_max_value_idx']}\")\n",
    "                \n",
    "        i = hole_info[\"R_threshold_cross\"] + 1\n",
    "        hole_counter['confirmed'] += 1\n",
    "        #endregion\n",
    "    \n",
    "    print(f\"Returning: {len(magnetic_holes)} holes, {len(hole_minima)} minima, {len(hole_maxima_pairs)} max pairs\")\n",
    "    return magnetic_holes, hole_minima, hole_maxima_pairs, times_clipped, bmag,  magnetic_hole_details\n",
    "\n",
    "#-------- \u2699\ufe0f Constants--------//\n",
    "#region Constants Block\n",
    "INSTRUMENT_SAMPLING_RATE = 292.9  # Will not be used if use_calculated_sampling_rate = 1\n",
    "use_calculated_sampling_rate = 1\n",
    "depth_percentage_threshold = .25\n",
    "smoothing_window_seconds = 8\n",
    "derivative_window_seconds = .2\n",
    "min_max_finding_smooth_window = .3\n",
    "mean_threshold = 0.8\n",
    "search_in_progress_output = True\n",
    "additional_seconds_for_min_search = 0.2\n",
    "asymetric_peak_threshold = .25\n",
    "symmetrical_peak_scan_window_in_secs = 2\n",
    "Bave_scan_seconds = .1\n",
    "Bave_window_seconds = 20\n",
    "wide_angle_threshold = 15\n",
    "small_threshold_cross_flag = 10\n",
    "small_threshold_cross_adjustment = 10\n",
    "#endregion\n",
    "\n",
    "smoothing_window_seconds = 8  # 8-second smoothing for primary detection\n",
    "derivative_window_seconds = .2  # Window for smoothing before calculating the first derivative\n",
    "\n",
    "# min_max_finding_smooth_window = .05  # 0.05 was being tried for a special case and seemed to work well...\n",
    "min_max_finding_smooth_window = .3  # 0.3-second smoothing for detailed max finding, this was the OG\n",
    "\n",
    "mean_threshold = 0.8  # 80% of the smoothed value\n",
    "search_in_progress_output = True  # Set to True for detailed output\n",
    "additional_seconds_for_min_search = 0.2  # Extend the search range\n",
    "\n",
    "# Define a threshold for peak asymmetry\n",
    "asymetric_peak_threshold = .25  # \ud83e\ude90 25% asymmetry threshold, was originally 10\n",
    "symmetrical_peak_scan_window_in_secs = 2  # Time window to scan for symmetrical peaks in seconds\n",
    "\n",
    "Bave_scan_seconds = .1\n",
    "\n",
    "# New constant for wide angle threshold\n",
    "Bave_window_seconds = 20  # Window size for calculating Bave and delta_B, this is where the angle is calculated\n",
    "wide_angle_threshold = 15  # 15 degrees threshold for W angle\n",
    "\n",
    "small_threshold_cross_flag = 10  # Define the small threshold cross (i.e. we only cross the threshold for _ samples)\n",
    "small_threshold_cross_adjustment = 10  # Define the adjustment when small threshold cross is detected (i.e. widen by _ to help our algorithm)\n",
    "#endregion\n",
    "\n",
    "# -------- \ud83e\udd17 File Output & Plotting Options--------//\n",
    "#region File Output & Plotting Options Block\n",
    "OUTPUT_PLOT = 1  # Set to 1 to output the plot\n",
    "plot_hole_minimum = 0  # Set to 1 to plot green lines at the minimum points within holes\n",
    "plot_smooth_threshold_crossing = 1  # Set to 1 to plot blue and purple lines at smooth threshold crossings (see mean_threshold defined below)\n",
    "SAVE_PLOT = 1 #Save the plot to the subdirectory\n",
    "\n",
    "\n",
    "OUTPUT_ZERO_CROSSING_PLOT = 0 #\u26f0\ufe0f\n",
    "\n",
    "IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN = 1  # Set to 1 to save iZotope formatted output with max and min indices to a .txt file\n",
    "MARKER_FILE_VERSION = 3  # Version number for marker file\n",
    "IZOTOPE_MARKER_FILE_OUTPUT = 0\n",
    "\n",
    "EXPORT_AUDIO_FILES = 1  # Set to 1 to export audio files\n",
    "AUDIO_SAMPLING_RATE = 22000  # Sampling rate for the audio files\n",
    "\n",
    "# New Option for Annotated Markers\n",
    "Marker_Files_With_Annotated_Markers = 0  # Set to 1 to include drop percentage and width in marker name\n",
    "Marker_Files_With_Hole_Numbers = 0\n",
    "\n",
    "SEARCH_IN_PROGRESS_OUTPUT = 1\n",
    "\n",
    "#-------- \ud83d\udcc8 Plotting Options --------//\n",
    "plot_hole_minimum = 1  # 1 to plot minima, 0 to skip\n",
    "plot_smooth_threshold_crossing = 1  # 1 to plot threshold crossings, 0 to skip\n",
    "#endregion\n",
    "\n",
    "#-------- \ud83c\uddf2\ud83c\udde6 Flag Handling--------//\n",
    "#region Flag Handling Block \n",
    "break_for_shallow_hole = 1 # \ud83c\uddf2\ud83c\udde6 flag for skipping if a hole is too shallow\n",
    "break_for_assymettry = 0 # \ud83c\uddf2\ud83c\udde6 flag for skipping if a whole is assymetrical\n",
    "break_for_wide_angle = 0  # \ud83c\uddf2\ud83c\udde6 flag for handling wide angle cases\n",
    "break_for_small_threshold_cross = 0  # \ud83c\uddf2\ud83c\udde6 flag to skip small threshold crosses\n",
    "break_for_complex_hole = 0 # \ud83c\uddf2\ud83c\udde6 flag for skipping complex holes\n",
    "threshold_for_derivative_0_crossings_flag = 1000 #If the .1s smoothed first derivative crosses this many times or higher, raise a flag\n",
    "break_for_derivative_crossings = 0\n",
    "#endregion\n",
    "\n",
    "#--------\u23f1\ufe0f Set time range--------//\n",
    "#region Time Range Block\n",
    "# Define the input parameters\n",
    "# trange = ['2023-09-28/06:10:00.000', '2023-09-28/07:10:00.000'] #Whole Shebang\n",
    "# trange = ['2023-09-28/06:37:15.000', '2023-09-28/06:37:45.000'] #wide lots o dips\n",
    "# trange = ['2023-09-28/06:37:05.500', '2023-09-28/06:37:08.000'] #Interesting Test Case WE'VE BEEN USING for Derivative\n",
    "trange = ['2023-09-28/06:39:00.000', '2023-09-28/06:40:10.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f GREAT region for solid hole testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2022-09-05/14:00:00.000', '2022-09-05/20:00:00.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f GREAT region for solid hole testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "\n",
    "# # # E13 CME Region\n",
    "# rangeStart = '2022/09/05 14:00:00.000'\n",
    "# rangeStop = '2022/09/05  20:00:00.000'\n",
    "\n",
    "# trange = ['2023-09-28/06:39:07.000', '2023-09-28/06:39:10.000'] #WONKY WONKS\n",
    "# trange = ['2023-09-28/06:39:50.000', '2023-09-28/06:39:55.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f Checking a single hole\n",
    "# trange = ['2023-09-28/06:39:31.000', '2023-09-28/06:39:35.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f PERFECT Single hole for testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:39:45.000', '2023-09-28/06:39:50.000'] #\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f ANOTHER PERFECT Single hole for testing\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:37:05.500', '2023-09-28/06:37:08.000'] #Interesting Test Case WE'VE BEEN USING for Derivative\n",
    "# trange = ['2023-09-28/06:36:022', '2023-09-28/06:36:24'] # A pretty hole \u2b50\ufe0f\n",
    "# trange = ['2023-09-28/06:29:18', '2023-09-28/06:29:22'] #with a larger window #\ud83c\uddf2\ud83c\udde6 Asymmetry between peaks: 42.25%\n",
    "#endregion\n",
    "\n",
    "time_check(trange)\n",
    "\n",
    "# At the beginning of your main script, after setting up save_dir\n",
    "sub_save_dir = setup_output_directory(trange, save_dir)\n",
    "print(f\"\ud83d\ude4c\ud83d\ude4c\ud83d\ude4cMain script: sub_save_dir set to {sub_save_dir}\")\n",
    "\n",
    "# # Call the function to detect magnetic holes\n",
    "# magnetic_holes, hole_minima, hole_maxima_pairs, times_clipped, bmag, magnetic_hole_details = detect_magnetic_holes(\n",
    "#     trange, \n",
    "#     smoothing_window_seconds, \n",
    "#     min_max_finding_smooth_window, \n",
    "#     mean_threshold, \n",
    "#     search_in_progress_output, \n",
    "#     Bave_scan_seconds, \n",
    "#     break_for_assymettry, \n",
    "#     small_threshold_cross_flag,          \n",
    "#     small_threshold_cross_adjustment,    \n",
    "#     break_for_small_threshold_cross,     \n",
    "#     break_for_complex_hole,\n",
    "#     derivative_window_seconds,  # Pass the derivative window into the function\n",
    "#     OUTPUT_ZERO_CROSSING_PLOT,\n",
    "#     SEARCH_IN_PROGRESS_OUTPUT,\n",
    "#     INSTRUMENT_SAMPLING_RATE,\n",
    "#     use_calculated_sampling_rate\n",
    "    \n",
    "# )\n",
    "\n",
    "# # Plotting the results if configured\n",
    "# if OUTPUT_PLOT:\n",
    "#     plot_mag_data_with_holes_and_minimum(\n",
    "#         times_clipped, \n",
    "#         bmag, \n",
    "#         magnetic_holes, \n",
    "#         hole_minima, \n",
    "#         hole_maxima_pairs, \n",
    "#         plot_hole_minimum, \n",
    "#         plot_smooth_threshold_crossing,\n",
    "#         trange,  # Add this\n",
    "#         save_dir,  # Add this\n",
    "#         SAVE_PLOT  # Add this if you don't want to save the plot by default\n",
    "#     )\n",
    "# # Create the settings dictionary\n",
    "# settings = {\n",
    "#     \"INSTRUMENT_SAMPLING_RATE\": INSTRUMENT_SAMPLING_RATE,\n",
    "#     \"use_calculated_sampling_rate\": use_calculated_sampling_rate,\n",
    "#     \"depth_percentage_threshold\": depth_percentage_threshold,\n",
    "#     \"smoothing_window_seconds\": smoothing_window_seconds,\n",
    "#     \"derivative_window_seconds\": derivative_window_seconds,\n",
    "#     \"min_max_finding_smooth_window\": min_max_finding_smooth_window,\n",
    "#     \"mean_threshold\": mean_threshold,\n",
    "#     \"search_in_progress_output\": search_in_progress_output,\n",
    "#     \"additional_seconds_for_min_search\": additional_seconds_for_min_search,\n",
    "#     \"asymetric_peak_threshold\": asymetric_peak_threshold,\n",
    "#     \"symmetrical_peak_scan_window_in_secs\": symmetrical_peak_scan_window_in_secs,\n",
    "#     \"Bave_scan_seconds\": Bave_scan_seconds,\n",
    "#     \"Bave_window_seconds\": Bave_window_seconds,\n",
    "#     \"wide_angle_threshold\": wide_angle_threshold,\n",
    "#     \"small_threshold_cross_flag\": small_threshold_cross_flag,\n",
    "#     \"small_threshold_cross_adjustment\": small_threshold_cross_adjustment,\n",
    "#     \"OUTPUT_PLOT\": OUTPUT_PLOT,\n",
    "#     \"plot_hole_minimum\": plot_hole_minimum,\n",
    "#     \"plot_smooth_threshold_crossing\": plot_smooth_threshold_crossing,\n",
    "#     \"OUTPUT_ZERO_CROSSING_PLOT\": OUTPUT_ZERO_CROSSING_PLOT,\n",
    "#     \"IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN\": IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN,\n",
    "#     \"MARKER_FILE_VERSION\": MARKER_FILE_VERSION,\n",
    "#     \"IZOTOPE_MARKER_FILE_OUTPUT\": IZOTOPE_MARKER_FILE_OUTPUT,\n",
    "#     \"EXPORT_AUDIO_FILES\": EXPORT_AUDIO_FILES,\n",
    "#     \"AUDIO_SAMPLING_RATE\": AUDIO_SAMPLING_RATE,\n",
    "#     \"Marker_Files_With_Annotated_Markers\": Marker_Files_With_Annotated_Markers,\n",
    "#     \"SEARCH_IN_PROGRESS_OUTPUT\": SEARCH_IN_PROGRESS_OUTPUT,\n",
    "#     \"break_for_shallow_hole\": break_for_shallow_hole,\n",
    "#     \"break_for_assymettry\": break_for_assymettry,\n",
    "#     \"break_for_wide_angle\": break_for_wide_angle,\n",
    "#     \"break_for_small_threshold_cross\": break_for_small_threshold_cross,\n",
    "#     \"break_for_complex_hole\": break_for_complex_hole,\n",
    "#     \"threshold_for_derivative_0_crossings_flag\": threshold_for_derivative_0_crossings_flag,\n",
    "#     \"break_for_derivative_crossings\": break_for_derivative_crossings\n",
    "# }\n",
    "\n",
    "# Get the sub-directory path\n",
    "# sub_save_dir = setup_output_directory(trange, save_dir)\n",
    "\n",
    "# last_dir_file = \"last_selected_dir.txt\"\n",
    "# save_dir = set_save_directory(last_dir_file)\n",
    "print(f'\ud83d\udedf save_dir = {save_dir}')\n",
    "\n",
    "# Use sub_save_dir for all file operations\n",
    "settings_file_path = os.path.join(sub_save_dir, 'magnetic_hole_detection_settings.json')\n",
    "with open(settings_file_path, 'w') as f:\n",
    "    json.dump(settings, f, indent=4)\n",
    "\n",
    "print(f\"Settings saved to: {settings_file_path}\")\n",
    "\n",
    "# Generate marker file if needed\n",
    "# if IZOTOPE_MARKER_FILE_OUTPUT or IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN:\n",
    "#     # Call the output_magnetic_holes function with the settings dictionary\n",
    "#     output_magnetic_holes(\n",
    "#         magnetic_holes,\n",
    "#         hole_maxima_pairs,\n",
    "#         times_clipped,\n",
    "#         bmag,\n",
    "#         IZOTOPE_MARKER_FILE_OUTPUT,\n",
    "#         IZOTOPE_MARKER_FILE_OUTPUT_MAX_AND_MIN,\n",
    "#         trange,\n",
    "#         MARKER_FILE_VERSION,\n",
    "#         SEARCH_IN_PROGRESS_OUTPUT,\n",
    "#         save_dir,\n",
    "#         INSTRUMENT_SAMPLING_RATE,\n",
    "#         Marker_Files_With_Annotated_Markers,\n",
    "#         Marker_Files_With_Hole_Numbers,\n",
    "#         magnetic_hole_details\n",
    "#     )\n",
    "\n",
    "# Export audio files if needed\n",
    "if EXPORT_AUDIO_FILES:\n",
    "    audify_high_res_mag_data_without_plot(trange[0], trange[1], sub_save_dir, AUDIO_SAMPLING_RATE, sub_save_dir)\n",
    "\n",
    "# # Summary print statements\n",
    "# print(\"\\n--- Magnetic Hole Detection Summary ---\")\n",
    "# print(f\"\ud83d\udd22 {hole_counter['potential']} Total potential holes identified\")\n",
    "# print(f\"\u2705 {hole_counter['confirmed']} Total holes confirmed\")\n",
    "# print(f\"\ud83c\udf00 {hole_counter['complex_holes']} Complex/Assymetrical holes\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_shallow_hole else '\ud83c\udd97'} {hole_counter['shallow']} Shallow holes\")\n",
    "# # print(f\"{'\u26d4\ufe0f' if break_for_assymettry else '\ud83c\udd97'} {hole_counter['asymmetric']} Asymmetric holes\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_wide_angle else '\ud83c\udd97'} {hole_counter['wide_angle']} Wide angle holes\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_small_threshold_cross else '\ud83c\udd97'} {hole_counter['small_threshold_cross']} Small threshold crosses\")\n",
    "# print(f\"{'\u26d4\ufe0f' if break_for_assymettry else '\ud83c\udd97'} {hole_counter['unresolved_asymmetry']} Unresolved asymmetric holes\")\n",
    "# print(f\"\ud83c\udd97 {hole_counter['asymmetric_initial']} Initially asymmetric holes (resolved)\")\n",
    "# # print(f\"\ud83c\udd97 {hole_counter['complex_resolved']} Complex holes (resolved)\")\n",
    "# # print(f\"{'\u26d4\ufe0f' if break_for_complex_hole else '\ud83c\udd97'} {hole_counter.get('complex', 0)} Complex holes\")\n",
    "# # print(f\"{'\u26d4\ufe0f' if break_for_derivative_crossings else '\ud83c\udd97'} {hole_counter['derivative_crossings']} Excessive derivative crossings\")\n",
    "# print(\"-------------------------------------\")\n",
    "\n",
    "# print(\"Settings have been saved to a JSON file in the sub-directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401af2bf-6ab8-4277-b923-87d2a6638528",
   "metadata": {},
   "outputs": [],
   "source": [
    "global save_dir\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894c61d-d886-4e30-86ed-dceb1a748940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trange storage:\n",
    "\n",
    "\n",
    "# trange = ['2023-09-28/06:37:23.000', '2023-09-28/06:37:27.000']\n",
    "# trange = ['2023-09-28/06:38:00.500', '2023-09-28/06:38:02.500']\n",
    "# trange = ['2023-09-28/06:36:00.500', '2023-09-28/06:38:00.500']\n",
    "# trange = ['2023-09-28/06:40:20', '2023-09-28/06:40:25']  # Region with a complex bump in the middle\n",
    "# trange = ['2023-09-28/06:32:50', '2023-09-28/06:32:52']  #Region wihere only one sample crossed threshold\n",
    "# trange = ['2023-09-28/06:39:28', '2023-09-28/06:40:03']  # AMAZING DREAM REGINO\n",
    "# trange = ['2023-09-28/06:29:20', '2023-09-28/06:29:22'] # \ud83c\uddf2\ud83c\udde6 Asymmetry between peaks: 42.25%\n",
    "# trange = ['2023-09-28/06:29:20', '2023-09-28/06:29:22'] # Complex Toss region\n",
    "# trange = ['2023-09-28/06:25:00', '2023-09-28/06:40:00'] # Long Full region\n",
    "# trange = ['2023-09-28/06:34:00', '2023-09-28/06:40:00'] # Long Full region Zooming in\n",
    "# trange = ['2023-09-28/06:36:35', '2023-09-28/06:37:00'] # GREAT Testing region\n",
    "# trange = ['2023-09-28/06:36:46', '2023-09-28/06:36:50'] # GREAT WILD Testing region, works with no before after avg, OMG\n",
    "# trange = ['2023-09-28/06:36:50', '2023-09-28/06:37:00'] #Also a WTF moment\n",
    "# trange = ['2023-09-28/06:37:00', '2023-09-28/06:37:20'] #Also a WTF moment\n",
    "# trange = ['2023-09-28/06:37:35', '2023-09-28/06:37:40'] #Two assymetrical holes and one not.\n",
    "# trange = ['2023-09-28/06:37:35', '2023-09-28/06:38:40']\n",
    "# trange = ['2023-09-28/06:38:15', '2023-09-28/06:38:25']\n",
    "# trange = ['2023-09-28/06:38:25', '2023-09-28/06:38:35']\n",
    "# trange = ['2023-09-28/06:38:00', '2023-09-28/06:39:00']\n",
    "# trange = ['2023-09-28/06:38:00', '2023-09-28/06:38:10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b963b-d2ee-41b1-ba68-5003afbc7941",
   "metadata": {},
   "source": [
    "## 2) Audify Electric Field Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd2a82-2ff2-4b9c-8467-8424b467c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audify Elect\n",
    "def convert_iso8601_to_str(trange_start, trange_stop):\n",
    "    \"\"\"\n",
    "    Convert the ISO 8601 time range to formatted strings.\n",
    "\n",
    "    Parameters:\n",
    "    trange_start (str): Start time in the format 'YYYY-MM-DDTHH:MM:SS'\n",
    "    trange_stop (str): Stop time in the format 'YYYY-MM-DDTHH:MM:SS'\n",
    "\n",
    "    Returns:\n",
    "    tuple: Formatted start and stop times as strings.\n",
    "    \"\"\"\n",
    "    start_datetime = datetime.strptime(trange_start, '%Y-%m-%dT%H:%M:%S')\n",
    "    stop_datetime = datetime.strptime(trange_stop, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    start_datetime_str = start_datetime.strftime('%Y_%m_%d_%Hh_%Mm_%Ss')\n",
    "    stop_datetime_str = stop_datetime.strftime('%Y_%m_%d_%Hh_%Mm_%Ss')\n",
    "    return start_datetime_str, stop_datetime_str\n",
    "\n",
    "\n",
    "def download_and_audify_efield(trange, save_dir, fsAud=22050):\n",
    "    # Convert time range to the ISO 8601 format strings\n",
    "    trange_start = trange[0].replace('/', 'T')\n",
    "    trange_stop = trange[1].replace('/', 'T')\n",
    "    \n",
    "    # Download electric field data using pyspedas\n",
    "    efield_data = pyspedas.psp.fields(trange=trange, datatype='dfb_wf_dvdc', level='l2', time_clip=False, get_support_data=True, notplot=True)\n",
    "    \n",
    "    # Check if data was downloaded successfully\n",
    "    if 'psp_fld_l2_dfb_wf_dVdc_sensor' not in efield_data:\n",
    "        print(\"Electric field data download failed.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract electric field data\n",
    "    efield_sensor = efield_data['psp_fld_l2_dfb_wf_dVdc_sensor']\n",
    "    times = efield_sensor['x']  # Time data\n",
    "    data_values = efield_sensor['y']  # Sensor electric field data with two columns\n",
    "    \n",
    "    # Convert time strings to datetime64 objects\n",
    "    start_time = np.datetime64(trange_start)\n",
    "    end_time = np.datetime64(trange_stop)\n",
    "    \n",
    "    # Clip the data to the specified time range\n",
    "    time_mask = (times >= start_time) & (times <= end_time)\n",
    "    \n",
    "    clipped_times = times[time_mask]\n",
    "    clipped_data_values = data_values[time_mask]\n",
    "\n",
    "    # Print statements for debugging\n",
    "    print(\"Clipped Times (first 10):\", clipped_times[:10])\n",
    "    print(\"Clipped Sensor Data Values (first 10):\", clipped_data_values[:10])\n",
    "    print(\"Clipped Shape of Data Values:\", clipped_data_values.shape)\n",
    "    \n",
    "    # Normalize the data to 16-bit integer format for audio\n",
    "    audio_data_component_1 = normalize_to_int16(clipped_data_values[:, 0])\n",
    "    audio_data_component_2 = normalize_to_int16(clipped_data_values[:, 1])\n",
    "    \n",
    "    # Create filenames based on the time range\n",
    "    start_datetime_str, stop_datetime_str = convert_iso8601_to_str(trange_start, trange_stop)\n",
    "    start_date, start_time = format_time(start_datetime_str)\n",
    "    stop_date, stop_time = format_time(stop_datetime_str)\n",
    "    \n",
    "    if start_date == stop_date:\n",
    "        stop_time_formatted = stop_time\n",
    "    else:\n",
    "        stop_time_formatted = f\"{stop_date}_{stop_time}\"\n",
    "\n",
    "    encounter_number = get_encounter_number(start_date)\n",
    "    \n",
    "    filename_component_1 = os.path.join(save_dir, f\"{encounter_number}_PSP_FIELDS_EField_Sensor_Component_1_{start_date}_{start_time}_to_{stop_time_formatted}.wav\")\n",
    "    filename_component_2 = os.path.join(save_dir, f\"{encounter_number}_PSP_FIELDS_EField_Sensor_Component_2_{start_date}_{start_time}_to_{stop_time_formatted}.wav\")\n",
    "    \n",
    "    # Write the audio files\n",
    "    write(filename_component_1, fsAud, audio_data_component_1)\n",
    "    write(filename_component_2, fsAud, audio_data_component_2)\n",
    "    \n",
    "    print(f\"Electric field sensor component 1 audio saved: {filename_component_1}\")\n",
    "    print(f\"Electric field sensor component 2 audio saved: {filename_component_2}\")\n",
    "    \n",
    "    return filename_component_1, filename_component_2\n",
    "\n",
    "# Example usage\n",
    "trange = ['2023-09-28/06:32:00', '2023-09-28/06:45:00']  # E17 Region Zooming\n",
    "download_and_audify_efield(trange, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c0568-3fdf-46af-8e22-4a6ead723b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdflib\n",
    "\n",
    "def inspect_cdf_file(cdf_file_path):\n",
    "    cdf = cdflib.CDF(cdf_file_path)\n",
    "    \n",
    "    # Get the CDF information\n",
    "    cdf_info = cdf.cdf_info()\n",
    "    \n",
    "    # Print the general info\n",
    "    print(\"CDF General Info:\")\n",
    "    print(cdf_info)\n",
    "    \n",
    "    # Access the list of zVariables (data variables)\n",
    "    z_variables = cdf_info.zVariables\n",
    "    \n",
    "    print(\"\\nVariables in the CDF file:\")\n",
    "    for var in z_variables:\n",
    "        print(f\"Variable: {var}\")\n",
    "        print(cdf.varinq(var))  # Information about each variable\n",
    "\n",
    "# Path to the downloaded CDF file\n",
    "cdf_file_path = 'psp_data/fields/l2/dfb_wf_dvdc/2023/psp_fld_l2_dfb_wf_dvdc_2023092806_v03.cdf'\n",
    "inspect_cdf_file(cdf_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527c3e8-fd17-4204-8a52-69afe50ee5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARKER FILE IMPORT MAGIC \ud83e\ude84 (UNDER CONSTRUCTION \ud83e\uddba)\n",
    "\n",
    "# Output options\n",
    "PRINT_EVERY_N_ITERATIONS = 100  # Number of iterations after which to print debug information\n",
    "\n",
    "OUTPUT_PLOT = 0  # Set to 1 to output the plot\n",
    "\n",
    "SEARCH_IN_PROGRESS_OUTPUT = 0  # Set to 1 to enable \"Search In Progress\" output\n",
    "DETAILED_OUTPUT = 1  # Set to 1 to enable detailed output on Magnetic Holes\n",
    "IZOTOPE_FORMATTED_OUTPUT = 0  # Set to 1 to enable iZotope formatted output to be PRINTED\n",
    "IZOTOPE_MARKER_FILE_OUTPUT = 1  # Set to 1 to save iZotope formatted output to a .txt file\n",
    "MARKER_FILE_VERSION = 2  # Version number for marker file\n",
    "EXPORT_AUDIO_FILES = 0  # Set to 1 to export audio files\n",
    "AUDIO_SAMPLING_RATE = 22000  # Sampling rate for the audio files\n",
    "\n",
    "import pandas as pd\n",
    "import pyspedas\n",
    "from pytplot import store_data\n",
    "import os\n",
    "from scipy.io.wavfile import write\n",
    "from datetime import datetime\n",
    "from tkinter import filedialog, Tk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def import_marker_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog to select a marker file.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Marker File\", filetypes=((\"Text files\", \"*.txt\"), (\"All files\", \"*.*\")))\n",
    "    root.destroy()  # Destroy the root window\n",
    "    return file_path\n",
    "\n",
    "def process_marker_file(file_path):\n",
    "    settings = {}\n",
    "    magnetic_holes = []\n",
    "    trange = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            print(f\"Debug: Processing line: {line.strip()}\")  # Debug print statement\n",
    "            parts = line.strip().split('\\t')\n",
    "            print(f\"Debug: Split parts: {parts}\")  # Debug print statement\n",
    "\n",
    "            if parts[0].startswith(\"[Metadata\"):\n",
    "                key_value = parts[2].split('=')\n",
    "                if len(key_value) == 2:\n",
    "                    key = key_value[0].strip()\n",
    "                    value = key_value[1].strip()\n",
    "                    settings[key] = value\n",
    "                    if parts[0] == \"[Metadata/trange]\":\n",
    "                        trange = value.strip(\"[]\").replace(\"'\", \"\").split(', ')\n",
    "            elif parts[0].startswith(\"MH\"):\n",
    "                match = re.match(r\"MH \\d+\\t(\\d+)\\t(\\d+)\", line)\n",
    "                if match:\n",
    "                    start_idx = int(match.group(1))\n",
    "                    end_idx = int(match.group(2))\n",
    "                    print(f\"Debug: Extracted indices - Start: {start_idx}, End: {end_idx}\")  # Debug print statement\n",
    "                    magnetic_holes.append((start_idx, end_idx))\n",
    "\n",
    "    return settings, magnetic_holes, trange\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = import_marker_file()\n",
    "    if file_path:\n",
    "        settings, magnetic_holes, trange = process_marker_file(file_path)\n",
    "        \n",
    "        # Use the extracted trange to download the high-resolution magnetic data\n",
    "        times, b_values = download_and_prepare_high_res_mag_data(trange)\n",
    "        \n",
    "        if b_values is None:\n",
    "            print(\"No magnetic field data available for the given time range.\")\n",
    "        else:\n",
    "            if SEARCH_IN_PROGRESS_OUTPUT:\n",
    "                for start_idx, min_idx, end_idx in magnetic_holes:\n",
    "                    print(f\"Confirmed magnetic hole from {start_idx} to {end_idx}\")\n",
    "\n",
    "            if DETAILED_OUTPUT:\n",
    "                for i, (start_idx, min_idx, end_idx) in enumerate(magnetic_holes):\n",
    "                    start_value = b_values[start_idx]\n",
    "                    min_value = b_values[min_idx]\n",
    "                    end_value = b_values[end_idx]\n",
    "                    left_peak = b_values[start_idx]\n",
    "                    right_peak = b_values[end_idx]\n",
    "                    average_peak = (left_peak + right_peak) / 2\n",
    "                    percentage_decrease = (1 - (min_value / average_peak)) * 100\n",
    "                    hole_width = end_idx - start_idx  # Calculate the width of the hole in samples\n",
    "\n",
    "                    print(f\"Magnetic Hole {i+1}:\")\n",
    "                    print(f\"  Start index: {start_idx} ({times[start_idx]})\")\n",
    "                    print(f\"  Start value: {start_value}\")\n",
    "                    print(f\"  Minimum index: {min_idx} ({times[min_idx]})\")\n",
    "                    print(f\"  Minimum value: {min_value}\")\n",
    "                    print(f\"  End index: {end_idx} ({times[end_idx]})\")\n",
    "                    print(f\"  End value: {end_value}\")\n",
    "                    print(f\"  Percentage Decrease: {percentage_decrease:.2f}%\")\n",
    "                    print(f\"  Width: {hole_width} samples\")\n",
    "\n",
    "            if OUTPUT_PLOT:\n",
    "                plot_mag_data_with_holes(times, b_values, magnetic_holes)\n",
    "\n",
    "            if EXPORT_AUDIO_FILES:\n",
    "                audify_high_res_mag_data_without_plot(trange[0], trange[1], save_dir, AUDIO_SAMPLING_RATE)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a67e47-3453-45dc-ad91-1122be703f7a",
   "metadata": {},
   "source": [
    "## 2) Download Protons, Store Variables, Calculate and define common plasma parameters and Electron PADs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c336d5bd-1ad1-43cf-b857-ddc50de4d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\u2705 #Download Proton Data And Rename Variables, and calculate common plasma parameters and PADs\n",
    "#Note that pyspedas currently loads in both the alphas (data product sf0a) and protons (sf00) with the same tplot variable name, \n",
    "#so we need to rename the alphas and proton tplot variables with different names, otherwise one will overwrite the other. \n",
    "#Pyspedas and pytplot is still in development, so this may change in the future.\n",
    "#pyspedas psp readme: https://pyspedas.readthedocs.io/en/latest/psp.html\n",
    "#tplot readme: https://pytplot.readthedocs.io/en/stable/index.html\n",
    "\n",
    "mag_datatype = 'mag_rtn_4_sa_per_cyc' # Set the datatype for magnetic field data\n",
    "#mag_datatype = 'mag_rtn' # Set the datatype for magnetic field data \n",
    "mag_vars = pyspedas.psp.fields(trange=trange, datatype=mag_datatype, level='l2', time_clip=True, get_support_data=True) # Download magnetic field data\n",
    "\n",
    "# Protons\n",
    "# Specify the span-i data type to plot\n",
    "# spi = solar probe instrument, sf00 = protons\n",
    "spi_sf00_datatype='spi_sf00_l3_mom' # Set the datatype for proton data\n",
    "spi_sf00_vars = pyspedas.psp.spi(trange=trange, datatype=spi_sf00_datatype, level='l3', time_clip=True) # Download proton data\n",
    "\n",
    "#rename tplot variables to distinguish proton variable name from alpha variable name\n",
    "tplot_rename(\"psp_spi_QUALITY_FLAG\",\"proton_QUALITY_FLAG\")\n",
    "tplot_rename(\"psp_spi_DENS\",\"proton_DENS\")\n",
    "tplot_rename(\"psp_spi_VEL_INST\",\"proton_VEL_INST\")\n",
    "tplot_rename(\"psp_spi_VEL_SC\",\"proton_VEL_SC\")\n",
    "tplot_rename(\"psp_spi_VEL_RTN_SUN\",\"proton_VEL_RTN_SUN\")\n",
    "tplot_rename(\"psp_spi_T_TENSOR_INST\",\"proton_T_TENSOR_INST\")\n",
    "tplot_rename(\"psp_spi_TEMP\",\"proton_TEMP\")\n",
    "tplot_rename(\"psp_spi_EFLUX_VS_ENERGY\",\"proton_EFLUX_VS_ENERGY\")\n",
    "tplot_rename(\"psp_spi_EFLUX_VS_THETA\",\"proton_EFLUX_VS_THETA\")\n",
    "tplot_rename(\"psp_spi_EFLUX_VS_PHI\",\"proton_EFLUX_VS_PHI\")\n",
    "tplot_rename(\"psp_spi_SUN_DIST\",\"proton_SUN_DIST\")\n",
    "tplot_rename(\"psp_spi_VENUS_DIST\",\"proton_VENUS_DIST\")\n",
    "tplot_rename(\"psp_spi_SC_VEL_RTN_SUN\",\"proton_SC_VEL_RTN_SUN\")\n",
    "tplot_rename(\"psp_spi_QUAT_SC_TO_RTN\",\"proton_QUAT_SC_TO_RTN\")\n",
    "tplot_rename(\"psp_spi_MAGF_SC\",\"proton_MAGF_SC\")\n",
    "tplot_rename(\"psp_spi_MAGF_INST\",\"proton_MAGF_INST\")\n",
    "\n",
    "#---------------------------------------------------------//\n",
    "# \u2705 Store |B|, Btn, |Vsw|, Vtn, Vr\n",
    "# Split magnetic field vector into 3 separate tplot variables (xyz = rtn)\n",
    "\n",
    "global save_dir, trange, trange_start, trange_stop\n",
    "\n",
    "print(trange_start,trange_stop)\n",
    "\n",
    "split_vec('psp_fld_l2_mag_RTN_4_Sa_per_Cyc')\n",
    "\n",
    "#access data in magnetic field components\n",
    "br = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_x')\n",
    "#note that times are stored in arrays as br.times and br.y for the values\n",
    "bt = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_y')\n",
    "bn = get_data('psp_fld_l2_mag_RTN_4_Sa_per_Cyc_z')\n",
    "#define |B| and store as tplot variable\n",
    "bmag = np.sqrt(br.y**2 + bt.y **2 + bn.y**2)\n",
    "\n",
    "store_data('|B|',data = {'x': br.times, 'y': bmag})\n",
    "options('|B|', 'color', ['green'])\n",
    "#Separate T and N components on one panel\n",
    "store_data('Btn', data = ['psp_fld_l2_mag_RTN_4_Sa_per_Cyc_y','psp_fld_l2_mag_RTN_4_Sa_per_Cyc_z'])\n",
    "options('Btn', 'color', ['red', 'blue']) \n",
    "options('Btn', 'legend_names', ['Bt', 'Bn'])\n",
    "options('Btn','ytitle','Btn')\n",
    "tplot_rename(\"psp_fld_l2_mag_RTN_4_Sa_per_Cyc_x\",\"Br\")\n",
    "options('Br','ytitle','Br')\n",
    "options('Br','color','black')\n",
    "\n",
    "#Split velocity vector into 3 separate tplot variables (xyz = rtn)\n",
    "split_vec('proton_VEL_RTN_SUN')\n",
    "\n",
    "#access data in velocity components\n",
    "vr = get_data('proton_VEL_RTN_SUN_x')\n",
    "vt = get_data('proton_VEL_RTN_SUN_y')\n",
    "vn = get_data('proton_VEL_RTN_SUN_z')\n",
    "#define |Vsw| and store as tplot variable\n",
    "vmag = np.sqrt(vr.y**2 + vt.y **2 + vn.y**2)\n",
    "store_data('|Vsw|',data = {'x': vr.times, 'y': vmag})\n",
    "#Separate T and N components on one panel\n",
    "store_data('Vtn', data = ['proton_VEL_RTN_SUN_y','proton_VEL_RTN_SUN_z'])\n",
    "options('Vtn', 'color', ['red', 'blue'])\n",
    "options('Vtn', 'legend_names', ['Vt', 'Vn'])\n",
    "options('Vtn','ytitle','Vtn')\n",
    "tplot_rename(\"proton_VEL_RTN_SUN_x\",\"Vr\")\n",
    "options('Vr','ytitle','Vr')\n",
    "options('Vr','color','black')\n",
    "\n",
    "#---------------------------------------------------------//\n",
    "#\u2705 Calculate Proton temperature anisotropy from Temperature Tensor, Tperp and Tparallel\n",
    "\n",
    "#Access tensor elements\n",
    "T_Tens = get_data('proton_T_TENSOR_INST')\n",
    "T_XX = T_Tens.y[:,0]\n",
    "T_YY = T_Tens.y[:,1]\n",
    "T_ZZ = T_Tens.y[:,2]\n",
    "T_XY = T_Tens.y[:,3]\n",
    "T_XZ = T_Tens.y[:,4]\n",
    "T_YZ = T_Tens.y[:,5]\n",
    "\n",
    "T_YX = T_XY\n",
    "T_ZX = T_XZ\n",
    "T_ZY = T_YZ\n",
    "\n",
    "#Access magnetic field in span-I coordinates\n",
    "B_spi = get_data('proton_MAGF_INST')\n",
    "B_X = B_spi.y[:,0]\n",
    "B_Y = B_spi.y[:,1]\n",
    "B_Z = B_spi.y[:,2]\n",
    "B_mag_XYZ = np.sqrt(B_X**2 + B_Y**2 + B_Z**2)\n",
    "\n",
    "#Project Tensor onto B field, find perpendicular and parallel components\n",
    "T_parallel=[]\n",
    "T_perpendicular=[]\n",
    "Anisotropy=[]\n",
    "for i in range(len(B_X)):\n",
    "    Sum_1=B_X[i]*B_X[i]*T_XX[i]\n",
    "    Sum_2=B_X[i]*B_Y[i]*T_XY[i]\n",
    "    Sum_3=B_X[i]*B_Z[i]*T_XZ[i]\n",
    "    Sum_4=B_Y[i]*B_X[i]*T_YX[i]\n",
    "    Sum_5=B_Y[i]*B_Y[i]*T_YY[i]\n",
    "    Sum_6=B_Y[i]*B_Z[i]*T_YZ[i]\n",
    "    Sum_7=B_Z[i]*B_X[i]*T_ZX[i]\n",
    "    Sum_8=B_Z[i]*B_Y[i]*T_ZY[i]\n",
    "    Sum_9=B_Z[i]*B_Z[i]*T_ZZ[i]    \n",
    "    T_para=((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "    Trace_Temp=(T_XX[i]+T_YY[i]+T_ZZ[i])\n",
    "    T_perp=(Trace_Temp-T_para)/2.0\n",
    "    T_parallel.append((Sum_1+Sum_2+Sum_3+Sum_4+Sum_5+Sum_6+Sum_7+Sum_8+Sum_9)/(B_mag_XYZ[i])**2)\n",
    "    T_perpendicular.append(T_perp)\n",
    "    Anisotropy.append(T_perp/T_para)\n",
    "\n",
    "#store tplot variables\n",
    "store_data('Proton TA',data={'x':B_spi.times,'y': Anisotropy}) #ratio Tperp/Tpar <-- HELPFUL!\n",
    "store_data('Proton Tperp',data={'x':B_spi.times,'y': T_perpendicular})\n",
    "store_data('Proton Tpar',data={'x':B_spi.times,'y': T_parallel})\n",
    "store_data('Proton TperpTpar',data=['Proton Tperp', 'Proton Tpar'])\n",
    "options('Proton TperpTpar', 'color', ['hotpink', 'deepskyblue'])\n",
    "options('Proton TperpTpar', 'legend_names', ['Tperp', 'Tpar'])\n",
    "#options('Proton TperpTpar','yrange',[0,200])\n",
    "options('Proton TperpTpar','ytitle','Proton T\u22a5/T\u2225')\n",
    "\n",
    "\n",
    "#---------------------------------------------------------//\n",
    "#\u2705 Calculate electron strahl pitch-angle distributions (PAD)\n",
    "# AKA the ultimate tracer of field topology\n",
    "\n",
    "# Access electron strahl pitch-angle distribution (PAD)\n",
    "spe_datatype = 'spe_sf0_pad'\n",
    "spe_vars = pyspedas.psp.spe(trange=trange, datatype=spe_datatype, level='l3', time_clip=True, get_support_data=True)\n",
    "\n",
    "epad_data = get_data('psp_spe_EFLUX_VS_PA_E')\n",
    "epad_PA = get_data('psp_spe_PITCHANGLE')\n",
    "\n",
    "times = epad_data.times\n",
    "epad_vals = epad_data.y\n",
    "epad_PA_vals = epad_PA.y\n",
    "\n",
    "encounter_number = get_encounter_number(trange_start)\n",
    "print(f\"Encounter Number: {encounter_number}\")\n",
    "\n",
    "# Set the ebin based on the encounter number\n",
    "if encounter_number in ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9']:\n",
    "    ebin = 8\n",
    "else:\n",
    "    ebin = 12\n",
    "\n",
    "print(f\"Using ebin: {ebin}\") # Note for E1-E9, electron strahl energy bin is 8, for E10 and above, it's 12\n",
    "\n",
    "epad_strahl = epad_vals[:, :, ebin]\n",
    "store_data('E Strahl', data={'x': times, 'y': epad_strahl, 'v': epad_PA_vals})\n",
    "pytplot.options('E Strahl', 'Spec', 1)\n",
    "pytplot.options('E Strahl', 'Zlog', 1)\n",
    "\n",
    "pytplot.tplot_options('wsize', [1000, 800])\n",
    "pytplot.tplot_options('axis_font_size', 10)\n",
    "pytplot.tplot_options('yaxis_width', 2000)\n",
    "\n",
    "# Proton Energy\n",
    "options('proton_EFLUX_VS_ENERGY', 'ytitle', 'Proton Energy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbfcbf-ee55-45bd-ab8a-bfc4d6c58b47",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3) Create Multiplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8156064-d327-4503-a388-3207a442e096",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "global save_dir, trange, trange_start, trange_stop\n",
    "\n",
    "# rangeStart = '2023/03/16 02:41:00.000'\n",
    "# rangeStop = '2023/03/16 02:42:00.000'\n",
    "\n",
    "# # # #Encounter 17 MH\n",
    "# rangeStart = '2022/02/25 12:30:00.000'\n",
    "# rangeStop = '2022/02/25 12:34:00.000'\n",
    "\n",
    "# # # #Encounter 9 MH Zoom\n",
    "# rangeStart = '2021/08/10 00:33:00.000'\n",
    "# rangeStop = '2021/08/10 00:37:00.000'\n",
    "\n",
    "# # # Encounter 10 MH storms Mystery Region\n",
    "# rangeStart = '2021/11/22 00:30:00.000'\n",
    "# rangeStop = '2021/11/22 03:30:00.000'\n",
    "\n",
    "# # Encounter 10 MH storms Mystery Region Zoom\n",
    "# rangeStart = '2021/11/22 01:27:00.000'\n",
    "# rangeStop = '2021/11/22 01:42:00.000'\n",
    "\n",
    "# # # Encounter 15 HCS Crossing\n",
    "rangeStart = '2023/03/17 21:09:30.000'\n",
    "rangeStop = '2023/03/17 21:11:30.000'\n",
    "\n",
    "trange_start = convert_to_trange_format(rangeStart)\n",
    "trange_stop = convert_to_trange_format(rangeStop)\n",
    "trange = [trange_start, trange_stop]\n",
    "\n",
    "# Dictionary of variables and their inclusion status\n",
    "variables = {\n",
    "    '|B|': 1,\n",
    "    'Br': 1,\n",
    "    'Btn': 1,\n",
    "    'Vr': 1,\n",
    "    'Vtn': 1,\n",
    "    'E Strahl': 1,\n",
    "    'Proton TA': 1,\n",
    "    'Proton TperpTpar': 1,\n",
    "    'proton_EFLUX_VS_ENERGY': 0,\n",
    "    '|Vsw|': 0,\n",
    "    'proton_EFLUX_VS_PHI': 0,\n",
    "    'proton_EFLUX_VS_THETA': 0,\n",
    "    #'Br Spectrum': 0, Spectrum code is not currently up and running\n",
    "    #'Bt Spectrum': 0,\n",
    "    #'Bn Spectrum': 0,\n",
    "}\n",
    "\n",
    "# Select and plot the variables with inclusion status 1\n",
    "selected_vars = [var for var, include in variables.items() if include == 1]\n",
    "\n",
    "options('Proton TA', 'ytitle', 'p TA')\n",
    "options('proton_EFLUX_VS_ENERGY', 'ytitle', 'p EFlux/E')\n",
    "options('Proton TperpTpar', 'ytitle', 'p T\u22a5/\u2225')\n",
    "options('E Strahl', 'ytitle', 'E Strahl')\n",
    "\n",
    "xlim(trange_start, trange_stop)\n",
    "#xlim('2023-09-28/06:32:00','2023-09-28/06:45:00') #or select a new sub-range\n",
    "\n",
    "# Generate the multiplot title\n",
    "plot_title = set_multiplot_title(trange_start, trange_stop)\n",
    "\n",
    "# Set the title of the multiplot\n",
    "tplot_options('title', plot_title)\n",
    "\n",
    "tplot(selected_vars)\n",
    "\n",
    "# Display the button to save the plot\n",
    "save_multiplot_button(trange, selected_vars)\n",
    "show_directory_button(save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7469fc-bf2d-4012-95df-9afa64863329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Change The Global Save Directory\n",
    "#Running this code will trigger a pop-up menu that will ask you to specify a directory where you'd like to save your files.\n",
    "#If you don't see a pop-up check your dock for a bouncing white python page and click that.\n",
    "#Each time you change this directory you'll need to re-run the cells that generate plots before they'll recognize the new location.\n",
    "#If this directory is not specified the code will not run properly.\n",
    "\n",
    "global save_dir\n",
    "last_dir_file = \"last_selected_dir.txt\"  # Path to save the last selected directory\n",
    "save_dir = set_save_directory(last_dir_file)\n",
    "  # Get the save directory, this c\n",
    "print('save_dir', save_dir)\n",
    "show_directory_button(save_dir) # Display the button to show the directory\n",
    "\n",
    "#\ud83d\udc47 Your save directory is confirmed here and you can click the button to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7ebf4-9e03-497a-b56a-93c4cbff286d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspedas\n",
    "from pyspedas import time_string, time_double\n",
    "import pytplot\n",
    "from pytplot import get_data, store_data, options, tplot\n",
    "import datetime\n",
    "import bisect\n",
    "\n",
    "# Define some constants and thresholds for MH detection\n",
    "DEPTH_THRESHOLD = 0.999  # Define a threshold for the depth of the magnetic hole (relative decrease in B-field)\n",
    "MIN_DURATION = .01  # Minimum duration of magnetic hole in seconds\n",
    "\n",
    "def download_mag_data(trange):\n",
    "    pyspedas.psp.fields(trange=trange, datatype='mag_rtn_4_sa_per_cyc', level='l2', time_clip=True, get_support_data=True)\n",
    "    split_vec('psp_fld_l2_mag_RTN_4_Sa_per_Cyc')\n",
    "\n",
    "def detect_magnetic_holes(trange, depth_threshold=DEPTH_THRESHOLD, min_duration=MIN_DURATION):\n",
    "    # Ensure magnetic data is downloaded\n",
    "    download_mag_data(trange)\n",
    "    \n",
    "    # Retrieve magnetic field data\n",
    "    b_data = get_data('|B|')\n",
    "    if b_data is None:\n",
    "        print(\"No magnetic field data available for the given time range.\")\n",
    "        return []\n",
    "\n",
    "    times, b_values = b_data.times, b_data.y\n",
    "\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    times = np.array(times)\n",
    "    b_values = np.array(b_values)\n",
    "\n",
    "    # Find magnetic holes\n",
    "    mh_indices = []\n",
    "    mh_depths = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(b_values) - 1:\n",
    "        # Find a potential start of a magnetic hole\n",
    "        if b_values[i] > (1 + depth_threshold) * b_values[i + 1]:\n",
    "            start = i\n",
    "            while i < len(b_values) - 1 and b_values[i] > (1 + depth_threshold) * b_values[i + 1]:\n",
    "                i += 1\n",
    "            end = i\n",
    "            if time_double(times[end]) - time_double(times[start]) >= min_duration:\n",
    "                mh_indices.append((start, end))\n",
    "                depth = (b_values[start] - b_values[end]) / b_values[start]\n",
    "                mh_depths.append(depth)\n",
    "        i += 1\n",
    "\n",
    "    return mh_indices, mh_depths\n",
    "\n",
    "def plot_magnetic_holes(trange, mh_indices):\n",
    "    # Retrieve magnetic field data for plotting\n",
    "    b_data = get_data('|B|')\n",
    "    times, b_values = b_data.times, b_data.y\n",
    "\n",
    "    # Plot magnetic field strength\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(times, b_values, label='|B|', color='black')\n",
    "\n",
    "    for start, end in mh_indices:\n",
    "        plt.axvspan(times[start], times[end], color='red', alpha=0.3)\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Magnetic Field Strength (nT)')\n",
    "    plt.title('Magnetic Holes Detection')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "trange = ['2021-08-10/00:33:00', '2021/08/10 00:37:00.000']\n",
    "mh_indices, mh_depths = detect_magnetic_holes(trange)\n",
    "\n",
    "if mh_indices:\n",
    "    print(f\"Detected {len(mh_indices)} magnetic holes:\")\n",
    "    for i, (start, end) in enumerate(mh_indices):\n",
    "        print(f\"Magnetic Hole {i+1}:\")\n",
    "        print(f\"  Start: {time_string(start)}\")\n",
    "        print(f\"  End: {time_string(end)}\")\n",
    "        print(f\"  Depth: {mh_depths[i]:.2f}\")\n",
    "\n",
    "    plot_magnetic_holes(trange, mh_indices)\n",
    "else:\n",
    "    print(\"No magnetic holes detected in the given time range.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e4ad9-c0e3-47a0-a6f3-4456c7f12622",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.) Plot Proton VDFs (Note From Robert: Nothing changed from here down)\n",
    "\n",
    "Go here for full tutorial: https://github.com/jlverniero/PSP_Data_Analysis_Tutorials/blob/main/PSP_SPAN-I_VDF_Plot_Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b836d4-d927-427a-828d-dc00f09a16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Define the function to convert the date string to the desired format\n",
    "def convert_to_trange_format(date_str):\n",
    "    date_obj = dt.datetime.strptime(date_str, '%Y/%m/%d %H:%M:%S.%f')\n",
    "    return date_obj.strftime('%Y-%m-%d/%H:%M:%S')\n",
    "\n",
    "# My timing code\n",
    "# Jaye E8 Region Zoomed\n",
    "# rangeStart = '2021/04/29 08:02:00.000'\n",
    "# rangeStop = '2021/04/29 08:20:00.000'\n",
    "\n",
    "# rangeStart = '2022/09/06 17:35:00.000'\n",
    "# rangeStop = '2022/09/06 17:55:00.000'\n",
    "\n",
    "# E17 Region\n",
    "# rangeStart = '2023/09/26 22:15:00.000'\n",
    "# rangeStop = '2023/09/26 22:30:00.000'\n",
    "\n",
    "trange = ['2022-09-28/06:35','2021-09-28/06:42']\n",
    "\n",
    "# rangeStart = '2023/09/28 06:35:00.000'\n",
    "# rangeStop = '2023/09/28 06:42:00.000'\n",
    "\n",
    "# Convert rangeStart and rangeStop to the desired format\n",
    "trange_start = convert_to_trange_format(rangeStart)\n",
    "trange_stop = convert_to_trange_format(rangeStop)\n",
    "trange = [trange_start, trange_stop]\n",
    "\n",
    "print(trange)\n",
    "\n",
    "# Call the helper function to download and process magnetic field data\n",
    "# Assuming download_and_process_mag_data is defined elsewhere and returns the converted trange\n",
    "trange_start, trange_stop = download_and_process_mag_data(rangeStart, rangeStop)\n",
    "\n",
    "# Set the global variable trange\n",
    "trange = [trange_start, trange_stop]  # Sets Time Range\n",
    "# ----/ End My timing Code\n",
    "\n",
    "# Use the trange in your PySPEDAS call\n",
    "spi_l2_vars = pyspedas.psp.spi(trange=trange, datatype='spi_sf00_8dx32ex8a', level='l2', time_clip=True, get_support_data=True, varnames=['*'], notplot=True, downloadonly=True)\n",
    "\n",
    "# Open CDF file\n",
    "dat = cdflib.CDF(spi_l2_vars[0])\n",
    "\n",
    "# Print variable names in CDF files\n",
    "print(dat._get_varnames())\n",
    "cdf_VDfile = dat\n",
    "\n",
    "# Check variable formats in CDF file\n",
    "print(cdf_VDfile)\n",
    "epoch_ns = cdf_VDfile['Epoch']\n",
    "theta = cdf_VDfile['THETA']\n",
    "phi = cdf_VDfile['PHI']\n",
    "energy = cdf_VDfile['ENERGY']\n",
    "eflux = cdf_VDfile['EFLUX']\n",
    "rotMat = cdf_VDfile['ROTMAT_SC_INST']\n",
    "counts = cdf_VDfile['DATA']\n",
    "\n",
    "# Convert time\n",
    "datetime_t0 = dt.datetime(2000, 1, 1, 12, 0, 0)\n",
    "epoch = cdflib.cdfepoch.to_datetime(cdf_VDfile.varget('Epoch'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e53dd-fee4-48c3-aedf-984fa5756354",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "import datetime as dt\n",
    "\n",
    "def convert_time(year, month, day, hour, minute, second):\n",
    "    # Convert time\n",
    "    tSlice = dt.datetime(year, month, day, hour, minute, second)\n",
    "    print('Desired timeslice start:', tSlice)\n",
    "\n",
    "    # Find span-i index for desired timeslice\n",
    "    # Ensure epoch contains datetime objects\n",
    "    tSliceIndex = bisect.bisect_left(epoch, tSlice)\n",
    "    print('time Index start spi:', tSliceIndex)\n",
    "    print('Time of closest start data point spi:', epoch[tSliceIndex])\n",
    "\n",
    "    return tSliceIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66060a8-5ec2-4e54-9bd8-64cf7d6ea0b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "import numpy as np\n",
    "\n",
    "def plot_spani_vdf(year, month, day, hour, minute, second):\n",
    "    # Input time, output theta and phi plane VDF plots\n",
    "    \n",
    "    # Find index of given time\n",
    "    tSliceIndex = convert_time(year, month, day, hour, minute, second)\n",
    "    \n",
    "    # Access data at that index\n",
    "    epochSlice = epoch[tSliceIndex]\n",
    "    thetaSlice = theta[tSliceIndex, :]\n",
    "    phiSlice = phi[tSliceIndex, :]\n",
    "    energySlice = energy[tSliceIndex, :]\n",
    "    efluxSlice = eflux[tSliceIndex, :]\n",
    "    countsSlice = counts[tSliceIndex, :]\n",
    "    \n",
    "    # Reshape data to reflect 8 theta bins, 32 energy bins, and 8 phi bins\n",
    "    thetaReshaped = thetaSlice.reshape((8, 32, 8))\n",
    "    phiReshaped = phiSlice.reshape((8, 32, 8))\n",
    "    energyReshaped = energySlice.reshape((8, 32, 8))\n",
    "    efluxReshaped = efluxSlice.reshape((8, 32, 8))\n",
    "    countsReshaped = countsSlice.reshape((8, 32, 8))\n",
    "\n",
    "    mass_p = 0.010438870  # Proton mass in units eV/c^2 where c = 299792 km/s\n",
    "    charge_p = 1  # Proton charge in units eV\n",
    "\n",
    "    # Define VDF\n",
    "    numberFlux = efluxReshaped / energyReshaped\n",
    "    vdf = numberFlux * (mass_p ** 2) / ((2E-5) * energyReshaped)\n",
    "\n",
    "    # Convert to velocity units in each energy channel\n",
    "    vel = np.sqrt(2 * charge_p * energyReshaped / mass_p)\n",
    "    vx = vel * np.cos(np.radians(phiReshaped)) * np.cos(np.radians(thetaReshaped))\n",
    "    vy = vel * np.sin(np.radians(phiReshaped)) * np.cos(np.radians(thetaReshaped))\n",
    "    vz = vel * np.sin(np.radians(thetaReshaped))\n",
    "\n",
    "    # Theta is along dimension 0, while phi is along 2\n",
    "    # First cut through theta\n",
    "    theta_cut = 0 \n",
    "\n",
    "    phi_plane = phiReshaped[theta_cut, :, :]\n",
    "    theta_plane = thetaReshaped[theta_cut, :, :]\n",
    "    energy_plane = energyReshaped[theta_cut, :, :]\n",
    "    vel_plane = np.sqrt(2 * charge_p * energy_plane / mass_p)\n",
    "\n",
    "    df_theta = np.nansum(vdf, axis=0)\n",
    "\n",
    "    vx_plane_theta = vel_plane * np.cos(np.radians(phi_plane)) * np.cos(np.radians(theta_plane))\n",
    "    vy_plane_theta = vel_plane * np.sin(np.radians(phi_plane)) * np.cos(np.radians(theta_plane))\n",
    "    vz_plane_theta = vel_plane * np.sin(np.radians(theta_plane))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cs = ax.contourf(vx_plane_theta, vz_plane_theta, df_theta, locator=ticker.LogLocator(), cmap=cm.cool)\n",
    "    cbar = fig.colorbar(cs)\n",
    "    cbar.set_label(f'f $(cm^2 \\\\ s \\\\ sr \\\\ eV)^{-1}$')\n",
    "\n",
    "    ax.set_xlim(-1000, 0)\n",
    "    ax.set_ylim(-500, 500)\n",
    "    ax.set_xlabel('$v_x$ km/s')\n",
    "    ax.set_ylabel('$v_z$ km/s')\n",
    "    tname = f\"{epoch[tSliceIndex].strftime('%y-%m-%d/%H:%M:%S')}\"\n",
    "    ax.set_title('Protons ' + tname, fontsize=12)\n",
    "    \n",
    "    # Now repeat for phi dimension\n",
    "    phi_cut = 1\n",
    "    \n",
    "    phi_plane = phiReshaped[:, :, phi_cut]\n",
    "    theta_plane = thetaReshaped[:, :, phi_cut]\n",
    "    energy_plane = energyReshaped[:, :, phi_cut]\n",
    "    vel_plane = np.sqrt(2 * charge_p * energy_plane / mass_p)\n",
    "\n",
    "    df_phi = np.nansum(vdf, axis=2)\n",
    "\n",
    "    vx_plane_phi = vel_plane * np.cos(np.radians(phi_plane)) * np.cos(np.radians(theta_plane))\n",
    "    vy_plane_phi = vel_plane * np.sin(np.radians(phi_plane)) * np.cos(np.radians(theta_plane))\n",
    "    vz_plane_phi = vel_plane * np.sin(np.radians(theta_plane))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    cs = ax.contourf(np.transpose(vx_plane_phi), np.transpose(vy_plane_phi), np.transpose(df_phi), locator=ticker.LogLocator(), cmap=cm.cool)\n",
    "    cbar = fig.colorbar(cs)\n",
    "    cbar.set_label(f'f $(cm^2 \\\\ s \\\\ sr \\\\ eV)^{-1}$')\n",
    "\n",
    "    ax.set_xlim(-700, 0)\n",
    "    ax.set_ylim(-200, 500)\n",
    "    ax.set_xlabel('$v_x$ km/s')\n",
    "    ax.set_ylabel('$v_y$ km/s')\n",
    "    ax.set_title('VDF SPAN-I $\\\\phi$-plane')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745750fd-e144-4dc7-984f-930d455923d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(type(epoch[0]))  # This should print <class 'datetime.datetime'>\n",
    "\n",
    "import bisect\n",
    "plot_spani_vdf(2023, 9, 28, 6, 35, 0)\n",
    "#trange: '2021-04-29/08:02:00', '2021-04-29/08:20:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c877920-df0c-49a3-9ae3-ba2250eb22b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of variables and their inclusion status\n",
    "variables = {\n",
    "    'Proton TperpTpar': 0,\n",
    "    'Proton TA': 1,\n",
    "    'Vr': 0,\n",
    "    'Vtn': 0,\n",
    "    'Btn': 0,\n",
    "    'Br': 0,\n",
    "    '|B|': 0,\n",
    "    'Br Spectrum':0,\n",
    "    'Bt Spectrum':1,\n",
    "    'Bn Spectrum':0,\n",
    "    'E Strahl': 1,\n",
    "    'proton_EFLUX_VS_ENERGY': 1\n",
    "}\n",
    "\n",
    "#xlim(trange_start,trange_stop)\n",
    "xlim('2021-04-29/08:14:00','2021-04-29/08:15:10')\n",
    "print(trange_start,trange_stop)\n",
    "\n",
    "# Select and plot the variables with inclusion status 1\n",
    "selected_vars = [var for var, include in variables.items() if include == 1]\n",
    "\n",
    "tplot(selected_vars)\n",
    "print(trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33fff5-9ff8-4e7e-98e4-0eb96bbbadb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.) Check for sufficent SPAN-I field-of-view (FOV)\n",
    "\n",
    "Go here for full tutorial: https://github.com/jlverniero/PSP_Data_Analysis_Tutorials/blob/main/PSP_SPAN-I_FOV_diagnostic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0e260-6543-4f21-8aa3-27c1601e058c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### 5.1) Phi angle coverage:\n",
    "\n",
    "The first (and perhaps most dominant) effect on SPAN-I data reliabiliy is how much the plasma is cut off by the heat shield. This is mostly in the phi direction of the instrument. To see this, we plot the energy flux in the three planes of the instrument: Energy, Theta (anode), and Phi (look direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fe212-3d43-4cba-a386-7016ed0c73ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#reset full time range\n",
    "# xlim('2021-01-14','2021-01-21')\n",
    "tplot(['|B|','proton_EFLUX_VS_ENERGY','proton_EFLUX_VS_THETA','proton_EFLUX_VS_PHI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2b826-5006-4a85-a9fe-71ef7a052b39",
   "metadata": {},
   "source": [
    "In the fourth panel, we see how much energy flux is captured in each $\\phi$ angle bin. To determine \"good\" coverage by eye, one should observe that there exists a peak in the range of angles, meaning the peak (or core) of the distribution is covered in the phi measurement plane. We also want to make sure this peak is located at the 160 degree mark or lower. Note that the 180 bin will always appear to have lower flux, which may be a deceptive peak.\n",
    "\n",
    "\n",
    "Anytime, the collapsed VDFs (E, theta, phi), approach the edges of the coverage, we have partial loss of VDF information. Theta and Energy coverage are also an important factor to consider, especially in later encounters.\n",
    "\n",
    "We may define a rudimentary FOV diagnostic based on the location of the maximum flux in the phi-plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f64bdc-a264-4122-b345-e4246ba8c3a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#define variables\n",
    "eflux_phi_data=get_data('proton_EFLUX_VS_PHI')\n",
    "times_unix=eflux_phi_data.times\n",
    "eflux = eflux_phi_data.y\n",
    "phi = eflux_phi_data.v\n",
    "\n",
    "print(trange)\n",
    "\n",
    "#determine phi angle with max eflux\n",
    "max_phi_ind = np.argmax(eflux, axis=1)\n",
    "max_phi = phi[0, max_phi_ind]\n",
    "\n",
    "#determine average phi\n",
    "avg_phi = np.average(eflux,axis=1)\n",
    "\n",
    "#define fov array\n",
    "tlen = times_unix.shape[0]\n",
    "phi_fov=np.ones(tlen)\n",
    "\n",
    "#set threshold 163.125 degrees\n",
    "phi_thresh = phi[0,1]\n",
    "\n",
    "#set fov to 0, meaning reliable, if less than 163.125\n",
    "for x in range(0,tlen):\n",
    "    if max_phi[x] < phi_thresh:\n",
    "        phi_fov[x]=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcf2d1d-a6d2-44b7-9f86-6cf3271e04ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Now we have array of zeros and ones indicating if the phi coverage is sufficient. We can mark the times with red xs below, where 1 means sufficient, and 0 means insufficient.\n",
    "\n",
    "In reality, this determination is not quantized or binary. This is a 0th order, HIGHLY rudimentary diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf2640-86da-4a04-8f99-45fd7bf9d2a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "store_data('max_phi',data={'x':times_unix,'y':max_phi})\n",
    "options('max_phi','color','k')\n",
    "\n",
    "store_data('phi_fov',data={'x':times_unix,'y':phi_fov})\n",
    "options('phi_fov','color','r')\n",
    "options('phi_fov','symbols',True)\n",
    "options('phi_fov','marker','+')\n",
    "\n",
    "store_data('EFLUX_VS_PHI_max',data=[\"proton_EFLUX_VS_PHI\",\"max_phi\"])\n",
    "\n",
    "#tplot(['EFLUX_VS_PHI_max','phi_fov'])\n",
    "tplot(['phi_fov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae3310-5cb9-4d47-8e89-568b6fce9401",
   "metadata": {},
   "source": [
    "(Note I am not sure why the colorbar is offset after overplotting, this is most likely a pyspedas bug.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a4bc40-c982-46b3-94a5-ebe1b93f54e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can see that the inbound mostly does not have sufficent coverage by the SPAN-I instrument. During these times, over half the VDF is not captured and therefore the plasma measurements are unreliable during these times. In principle, one could then look to SPC during these times, if available. The sweap instrument team is currently working on a joint SPC-SPAN data product and calibration efforts are ongoing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af3b9d-4d4e-49c5-9274-3b8ac68dea82",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xlim('2021-01-16','2021-01-18')\n",
    "tplot(['EFLUX_VS_PHI_max','phi_fov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5b516-7804-4787-b5fd-bda47bec73ee",
   "metadata": {},
   "source": [
    "We can see that the measurements start to slighly be reliable on the 16th and more on the 17th. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a77b4a-55a5-488a-b4ea-b6f2d0bc7e81",
   "metadata": {},
   "source": [
    "In general, SPAN-I has better reliability +/- a few days from perihelion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8674949-f8f7-4adb-a2c2-bca28956e441",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xlim('2021-01-14','2021-01-21')\n",
    "options('proton_TEMP','yrange',[0,150])\n",
    "tplot(['proton_SUN_DIST','|B|','Proton TperpTpar','proton_TEMP','EFLUX_VS_PHI_max','phi_fov'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edb45b-7d01-43c2-aac5-75c1b901a8ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2) QTN Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b50b89-ab8f-4702-ac6e-c774686ff157",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "\n",
    "The Quasithermal Noise (QTN) data from the FIELDS instrument is presently the most reliable measurement of density. See Moncuquet et al. (DOI: 10.3847/1538-4365/ab5a84) for more details.\n",
    "\n",
    " Since SPAN-I measures partial moments of the VDF, density is highly sensitive to inaccuracies. In addition to the $\\phi$ angular coverage assessment described above, the data user is encouraged to compare the SPAN-I measured density to the density derived from QTN. Note that this measurement is also prone to error, and there is currently no agreed upon value of density. Ongoing calibration efforts by the instrument team continue to converge closer to an answer. Below, we show how to compare the QTN and SPAN-I measured density.\n",
    "\n",
    "Note there is not a defined threshold for offsets, as the answer depends on the physical problem at hand. The user is encouraged to reach out to the SPAN-I instrument team for recommendations if the SPAN-I density in the time period of interest deviates strongly from the QTN.\n",
    "\n",
    "Future additions to this tutorial will show how to also compare with density measured by SPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74021a-f750-413e-a3e4-c8a0539a2e9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#note we already downloaded this above, but in case you want to skip here to check FOV first, one can redownload.\n",
    "qtn_datatype = 'sqtn_rfs_V1V2' #sqtn\n",
    "qtn_vars = pyspedas.psp.fields(trange=trange, datatype=qtn_datatype, level='l3', time_clip=True,get_support_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351494ec-51b5-465d-aeea-ecf2bd5b5d78",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "store_data(\"DENS_compare\",data=['proton_DENS','electron_density'])\n",
    "options('DENS_compare','legend_names',['Np','Ne'])\n",
    "options('DENS_compare','color',['hotpink','deepskyblue'])\n",
    "\n",
    "divide('proton_DENS','electron_density','Np/Ne')\n",
    "options('Np/Ne','ytitle','Np/Ne')\n",
    "options('Np/Ne','yrange',[1e-2,2])\n",
    "\n",
    "tplot(['proton_SUN_DIST','|B|','EFLUX_VS_PHI_max','phi_fov','DENS_compare','Np/Ne'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2650d9ed-9c6e-4fe4-b543-670d4f89a717",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We may observe that devations of SPAN-I measured proton density (Np) and QTN measured electron density (Ne) are most apparent on the inbound. This is the reason why the alpha-proton number density differed from the alpha-electron number density. Depending on context, the reliability of the alpha measurments may be highly contingent on reliability of proton measurements, especially for temperature.\n",
    "\n",
    "In addition, deviations from QTN do not always coincide with insufficient phi angle coverage (and vice versa). These two diagnostics should be used independantly to assess measurement reliability. However, if your region of interest has both insufficient phi coverage and also strong deviations from QTN, then one needs to excercise extreme caution and spell out all caveats if one uses the SPAN-I measurements during these times. Other considerations to consider, such as magnetic field variability, may be found in the FOV tutorial: https://github.com/jlverniero/PSP_Data_Analysis_Tutorials/blob/main/PSP_SPAN-I_FOV_diagnostic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04809c43-ad9c-43fe-8e5d-b99457fb96af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "In summary, days far from perihelion most likely do not have reliable proton measurements from SPAN-I. This does not mean that SPAN-I should not be used at all, only that extra caution should be made and one should spell out all caveats before making a scientific claim. For answering physics problems that only require proton measurements on a larger scale, unreliable span measurment times may be cautiously used (as a ballpark lower bound for example). But, for kinetic physics problems that are highly sensitive to measurement uncertainties, it is not recommended to use SPAN-I during unreliable measurement times. \n",
    "\n",
    "Note that the reliability of the alpha particle measurements may also be assessed by the same procedure above. As a general rule of thumb, if the protons are unreliable, then so are the alphas (but not always). In addition, the alphas should only in general be trusted if the alpha/proton number density is above 1 percent. Future work will further characterize the alpha reliability.\n",
    "\n",
    "For optimal scientific interpretation of noteworthy event, the reader is strongly encouraged to contact a member of the instrument team:<br>\n",
    "Roberto Livi rlivi@berkeley.edu <br>\n",
    "Ali Rahmati rahmati@berkeley.edu <br>\n",
    "Davin Larson davin@berkeley.edu <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e348531-b5a0-4b39-9864-d72ccd86b9ca",
   "metadata": {},
   "source": [
    "Thank you, and happy exploring :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
